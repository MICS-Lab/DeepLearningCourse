\documentclass[]{article}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

%opening
\title{Neural Networks Can Approximate Continuous Functions}
\author{Paul Dubois}
\date{}

\begin{document}

\maketitle

\begin{abstract}
	A clean Tex version of the proof seen during the lectures that neural networks with ReLU activation can approximate continuous functions.
\end{abstract}

\begin{definition}
	$\mathcal{C}\left( \left[ a,b \right] \right)$ is the set of real continuous functions on $\left[ a,b \right]$.
\end{definition}
\begin{definition}
	$\mathcal{P}\left( n,l \right)$ is the set of rectangle 1D-1D perceptrons with $l$ hidden layers and $n$ neurons in each hidden layer.
\end{definition}

\section{Theorem}

\section{Idea}

\section{Proof}


\end{document}
