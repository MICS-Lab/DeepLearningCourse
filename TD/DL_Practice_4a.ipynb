{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate architectures and advanced PyTorch tools\n",
    "## TD 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are essentially going to use the same `Food101` ([credit where it's due](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)) data, the same object `ImageDataset`, the same `DataLoader`.\n",
    "\n",
    "The code below is mainly a copy of the code from the previous TD, except that global variables are now defined separately and everything is wrapped in different functions. This is to make it easier to train the same model with different hyperparameters and architectures, etc ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those that can use their GPUs, all the necessary `.to(device)` are already in the code.\n",
    "\n",
    "If, for some reason, you encounter this error: `OutOfMemoryError: CUDA out of memory.`. It means that your GPU does not have enough memory to run the model. You can try to reduce the batch size, or the number of neurons in the network, or the number of layers in the network, or the number of filters in the convolutional layers, etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "_ = torch.manual_seed(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the `flush` parameter to `True` for all `print()` statements in `Python` by overriding the built-in `print()` function using the `functools.partial()` method. An example of this is:\n",
    "\n",
    "```py\n",
    "from functools import partial\n",
    "print = partial(print, flush=True)\n",
    "```\n",
    "\n",
    "We will use this to make sure that the outputs are printed in the correct order and at the correct time (for more info, check [this link](https://www.includehelp.com/python/flush-parameter-in-python-with-print-function.aspx))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "print = partial(print, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "\n",
    "# Setup device-agnostic code\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 2e-2\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_and_dataloaders(\n",
    "    batch_size: int = 4\n",
    ") -> tuple[\n",
    "    datasets.ImageFolder, \n",
    "    datasets.ImageFolder, \n",
    "    DataLoader, \n",
    "    DataLoader\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the training and test datasets into data loaders.\n",
    "    \"\"\"\n",
    "    data_dir = pathlib.Path(\"data\")\n",
    "    train_dir = data_dir / \"Food-3\" / \"train\"\n",
    "    test_dir = data_dir / \"Food-3\" / \"test\"\n",
    "\n",
    "    data_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(64, 64)),  # Resize the images to 64x64*\n",
    "            transforms.ToTensor()  # Convert the images to tensors\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_data = datasets.ImageFolder(\n",
    "        root=train_dir,  # target folder of images\n",
    "        transform=data_transform,  # transforms to perform on data (images)\n",
    "        target_transform=None  # transforms to perform on labels (if necessary)\n",
    "    ) \n",
    "\n",
    "    test_data = datasets.ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=data_transform\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,  # how many samples per batch?\n",
    "        shuffle=True  # shuffle the data?\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    ) # don't usually need to shuffle testing data\n",
    "\n",
    "\n",
    "    return train_data, test_data, train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load dataloaders in global variables\n",
    "TRAIN_DATASET, TEST_DATASET, TRAIN_DATALOADER, TEST_DATALOADER = get_datasets_and_dataloaders(BATCH_SIZE)\n",
    "\n",
    "# We actually don't really need to return the datasets, but it's nice to have them for reference. If you don't,\n",
    "# you can just return the dataloaders and find the datasets by calling TRAIN_DATALOADER.dataset or TEST_DATALOADER.dataset:\n",
    "print(TRAIN_DATALOADER.dataset == TRAIN_DATASET)\n",
    "print(TEST_DATALOADER.dataset == TEST_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_units=200):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(64*64*3, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: Net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_our_model() -> float:\n",
    "    # 0. Put model in eval mode\n",
    "    MODEL.eval()  # to remove stuff like dropout that's only going to be in the training part\n",
    "\n",
    "    # 1. Setup test accuracy value\n",
    "    test_acc: float = 0\n",
    "\n",
    "    # 2. Turn on inference context manager\n",
    "    with torch.no_grad():\n",
    "        # Loop through DataLoader batches\n",
    "        for X_test, y_test in TEST_DATALOADER:  # majuscule à X car c'est une \"matrice\", et y un entier\n",
    "            # a. Move data to device\n",
    "            X_test_flattened = X_test.view(-1, 64*64*3).to(DEVICE) \n",
    "            y_test = y_test.to(DEVICE)\n",
    "\n",
    "            # b. Forward pass\n",
    "            model_output = MODEL(X_test_flattened)\n",
    "\n",
    "            # c. Calculate and accumulate accuracy\n",
    "            test_pred_label = model_output.argmax(dim=1)\n",
    "            test_acc += (test_pred_label == y_test).sum()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_acc = test_acc / (len(TEST_DATASET))\n",
    "    return test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.00%\n"
     ]
    }
   ],
   "source": [
    "# Test our untrained model\n",
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 36.00% accuracy on the testing set without training and with the default hyperparameters if you used the same seed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Why does it not work with ` X_test_flattened = X_test.view(BATCH_SIZE, 64*64*3).to(DEVICE)`?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(loss_fn, optimizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and modified the trained model inplace.\n",
    "    \"\"\"\n",
    "    start_time_global = time.time()\n",
    "\n",
    "    # Put model in train mode\n",
    "    MODEL.train()\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Setup train loss and train accuracy values\n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for X, y in TRAIN_DATALOADER:\n",
    "            # 0. Move data to device\n",
    "            X = X.view(-1, 64*64*3).to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = MODEL(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 4. Loss backward\n",
    "            loss.backward()\n",
    "\n",
    "            # 5. Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = y_pred.argmax(dim=1)\n",
    "            train_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / (len(TRAIN_DATASET))\n",
    "        train_acc = train_acc / (len(TRAIN_DATASET))\n",
    "        print(\n",
    "            f\"epoch {epoch+1}/{NUM_EPOCHS},\"\n",
    "            f\" train_loss = {train_loss:.2e},\"\n",
    "            f\" train_acc = {100*train_acc.item():.2f}%,\"\n",
    "            f\" time spent during this epoch = {time.time() - start_time_epoch:.2f}s,\"\n",
    "            f\" total time spent = {time.time() - start_time_global:.2f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, train_loss = 1.25e-01, train_acc = 49.81%, time spent during this epoch = 6.92s, total time spent = 6.92s\n",
      "epoch 2/15, train_loss = 1.16e-01, train_acc = 54.41%, time spent during this epoch = 6.06s, total time spent = 12.99s\n",
      "epoch 3/15, train_loss = 1.12e-01, train_acc = 57.63%, time spent during this epoch = 6.46s, total time spent = 19.45s\n",
      "epoch 4/15, train_loss = 1.09e-01, train_acc = 59.33%, time spent during this epoch = 6.33s, total time spent = 25.78s\n",
      "epoch 5/15, train_loss = 1.08e-01, train_acc = 60.41%, time spent during this epoch = 6.32s, total time spent = 32.10s\n",
      "epoch 6/15, train_loss = 1.05e-01, train_acc = 60.67%, time spent during this epoch = 5.98s, total time spent = 38.08s\n",
      "epoch 7/15, train_loss = 1.03e-01, train_acc = 61.30%, time spent during this epoch = 5.98s, total time spent = 44.06s\n",
      "epoch 8/15, train_loss = 1.01e-01, train_acc = 62.48%, time spent during this epoch = 5.98s, total time spent = 50.04s\n",
      "epoch 9/15, train_loss = 9.84e-02, train_acc = 65.07%, time spent during this epoch = 6.02s, total time spent = 56.07s\n",
      "epoch 10/15, train_loss = 9.64e-02, train_acc = 65.11%, time spent during this epoch = 6.02s, total time spent = 62.09s\n",
      "epoch 11/15, train_loss = 9.54e-02, train_acc = 64.78%, time spent during this epoch = 5.97s, total time spent = 68.07s\n",
      "epoch 12/15, train_loss = 9.20e-02, train_acc = 67.11%, time spent during this epoch = 5.96s, total time spent = 74.02s\n",
      "epoch 13/15, train_loss = 8.93e-02, train_acc = 67.93%, time spent during this epoch = 6.01s, total time spent = 80.03s\n",
      "epoch 14/15, train_loss = 8.67e-02, train_acc = 69.00%, time spent during this epoch = 6.38s, total time spent = 86.41s\n",
      "epoch 15/15, train_loss = 8.42e-02, train_acc = 69.85%, time spent during this epoch = 6.28s, total time spent = 92.69s\n"
     ]
    }
   ],
   "source": [
    "main_train(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.67%\n"
     ]
    }
   ],
   "source": [
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 55.67% accuracy on the testing set without training and with the default hyperparameters if you used the same seed. And we almost reached convergence (the loss is not decreasing that much anymore, and if you try to train for more epochs, you will see that the testing set accuracy will decrease). Note that we kind of cheated by using the testing set to set the number of epochs, we should instead use validation sets and cross validation techniques ... and we will (today)! No worries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible for `train_loss` to decrease whilst `train_acc` decreases at the same time? Look at what happens between epochs 10 and 11 here:\n",
    "\n",
    "```\n",
    "epoch 10/15, train_loss = 9.64e-02, train_acc = 65.11%, [...], total time spent = 121.83s\n",
    "epoch 11/15, train_loss = 9.54e-02, train_acc = 64.78%, [...], total time spent = 134.67s\n",
    "```\n",
    "\n",
    "Why is that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to improve this accuracy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install the Optuna package (`pip install optuna`) and import it at the beginning of your script. We should also import KFold from sklearn.model_selection. This is because we will use cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First easy task is to decide whether one should use a convolutional network or a dense network.\n",
    " \n",
    " We will do this together (choice between a convolutional and dense network), and then you'll have to implement optimization of the learning rate* and optimizer's choice on your own.\n",
    "\n",
    " \\* *Careful! Small learning rates are not always better, especially if you do not change the number of epochs. You should try to find the best learning rate for the number of epochs you chose, one that is not too big for your computer to handle.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNet(nn.Module):\n",
    "    def __init__(self, use_conv: bool, hidden_units: int = 200):\n",
    "        super(AdvancedNet, self).__init__()\n",
    "        self.use_conv: bool = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "            # output of this layer will be ((64+2*1-3)/1)+1 = 64. \n",
    "            # -> 64 channels of 64x64 images\n",
    "            self.fc1 = nn.Linear(64*64*64, hidden_units)  # flattening will be necessary to enter fc1\n",
    "            self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(3*64*64, hidden_units)\n",
    "            self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_conv:\n",
    "            x = nn.ReLU()(self.conv(x))\n",
    "            x = x.view(-1, 64*64*64)  # flattening is necessary, and, same as above,\n",
    "            # we need to use -1 and not BATCH_SIZE because the last batch might be smaller\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, you will need to define a new function that will be used as the objective function for Optuna's optimization. This function should take in the `trial` object from Optuna as an argument and use the `trial` object to define and sample the hyperparameters that you want to optimize. For example, you can use the `trial` object to sample a choice between a convolutional and dense network, and to sample the number of neurons for the chosen network. After training the model, we will need to return the final validation accuracy calculated with cross-validation* as the objective function value for Optuna to maximise.\n",
    "\n",
    " \\* We use cross-validation here (3-fold) because we want to use the testing set as little as possible. We will use the testing set only once, at the end, to get the final accuracy of the best model. But, cross-validation greatly increases the time required to run the algorithms, so we won't always use cross-validation to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    print(\"New trial\")\n",
    "\n",
    "    # Set up cross validation\n",
    "    n_splits: int = 3\n",
    "    fold = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    scores = [0]*n_splits\n",
    "\n",
    "    use_conv: bool = trial.suggest_categorical('use_conv', [True, False])\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(TRAIN_DATASET)))):\n",
    "        # train_idx and valid_idx are numpy arrays of indices of the training and validation sets for this fold respectively.\n",
    "        # They do not contain the actual data, but the indices of the data in the dataset.\n",
    "        # We can use these indices to create a subset of the dataset for this fold with torch.utils.data.Subset.\n",
    "        # Obviously, if an index is in the validation set, it will not be in the training set. You can\n",
    "        # check this by printing train_idx and valid_idx and check by yourself.\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}/{n_splits}\")\n",
    "\n",
    "        # Create subsets of the dataset for this fold\n",
    "        sub_train_data = torch.utils.data.Subset(TRAIN_DATASET, train_idx)\n",
    "        sub_valid_data = torch.utils.data.Subset(TRAIN_DATASET, valid_idx)\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        sub_train_loader = torch.utils.data.DataLoader(sub_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        sub_valid_loader = torch.utils.data.DataLoader(sub_valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        # Generate the model.\n",
    "        my_model: AdvancedNet = AdvancedNet(use_conv).to(DEVICE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training of the model.\n",
    "            # Put model in train mode\n",
    "            my_model.train()\n",
    "\n",
    "            # Set up optimizer\n",
    "            optimizer = torch.optim.SGD(my_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            # Set up loss function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            for X, y in sub_train_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "\n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "\n",
    "                # 3. Calculate and accumulate loss\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                # 4. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 5. Loss backward\n",
    "                loss.backward()\n",
    "\n",
    "                # 6. Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        # Put model in eval mode\n",
    "        my_model.eval()\n",
    "        \n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in sub_valid_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "                \n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "                \n",
    "                # 3. Compute accuracy\n",
    "                pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "                y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "                val_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        scores[fold_idx] = (val_acc / len(sub_valid_data)).cpu()\n",
    "        # bring it back otherwise, np.mean will not work\n",
    "        print(f\"Fold {fold_idx+1}/{n_splits} accuracy: {scores[fold_idx]}\")\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will need to call the `optuna.create_study()` function to create a new study, and use the `study.optimize()` function to run the optimization, passing the objective function that we defined earlier.\n",
    "\n",
    "You can find more information about how to use Optuna in the [Optuna documentation](https://optuna.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 13:44:11,556]\u001b[0m A new study created in memory with name: no-name-7b673fbf-1ae0-4fcd-859e-ea34fb593c6b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6166666746139526\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.6344444751739502\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6422222256660461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 13:47:24,304]\u001b[0m Trial 0 finished with value: 0.6311111450195312 and parameters: {'use_conv': True}. Best is trial 0 with value: 0.6311111450195312.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6366666555404663\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.6277778148651123\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6333333253860474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 13:50:43,063]\u001b[0m Trial 1 finished with value: 0.6325926184654236 and parameters: {'use_conv': True}. Best is trial 1 with value: 0.6325926184654236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  2\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  2\n",
      "Best trial:\n",
      "  Value:  0.6325926184654236\n",
      "  Params: \n",
      "\tuse_conv: True\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=1200, n_trials = 2) \n",
    "# - timeout=1200 -> stops after 20 minutes; \n",
    "# - n_trials = 2 -> here we only try two models, a dense or a convolutional model so\n",
    "#   we need to make it stop after having trained the two models otherwise it will continue to \n",
    "#   loop on those two models unless it reaches the 20 minutes mark*. In practice, you will give\n",
    "#   a lot of hyperparameters to optimize and you will want to run the optimization for a lot\n",
    "#   longer than 20 minutes. The timeout parameter is useful in those cases because you won't \n",
    "#   know how long it'll take.\n",
    "#   * e.g., https://i.imgur.com/bCzH1pm.png\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of you lot might have a problem: we've only allowed two trials but `Optuna` tried `False` then `False` or `True` then `True`. This is because `Optuna` doesn't check if it already has used the previous set of hyperparameters. To fix this, we can add the following code:\n",
    "\n",
    "```py\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "...\n",
    "\n",
    "for previous_trial in trial.study.trials:\n",
    "    if previous_trial.state == TrialState.COMPLETE and trial.params == previous_trial.params:\n",
    "        print(f\"Duplicated trial: {trial.params}, return {previous_trial.value}\")\n",
    "        return previous_trial.value\n",
    "```\n",
    "\n",
    "And set n_trials to 5 for example, that way it'll be very unlikely to have the same hyperparameters twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 13:50:43,112]\u001b[0m A new study created in memory with name: no-name-9ad8c7f1-3000-42a1-a92e-ae1faac6f53c\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6155555844306946\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.6255555748939514\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6322222352027893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 13:54:01,482]\u001b[0m Trial 0 finished with value: 0.6244444847106934 and parameters: {'use_conv': True}. Best is trial 0 with value: 0.6244444847106934.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Duplicated trial: {'use_conv': True}, return 0.6244444847106934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 13:54:01,484]\u001b[0m Trial 1 finished with value: 0.6244444847106934 and parameters: {'use_conv': True}. Best is trial 0 with value: 0.6244444847106934.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.5877777934074402\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.5944444537162781\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6122222542762756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 14:06:58,015]\u001b[0m Trial 2 finished with value: 0.5981481671333313 and parameters: {'use_conv': False}. Best is trial 0 with value: 0.6244444847106934.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Duplicated trial: {'use_conv': True}, return 0.6244444847106934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 14:06:58,017]\u001b[0m Trial 3 finished with value: 0.6244444847106934 and parameters: {'use_conv': True}. Best is trial 0 with value: 0.6244444847106934.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Duplicated trial: {'use_conv': True}, return 0.6244444847106934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-24 14:06:58,020]\u001b[0m Trial 4 finished with value: 0.6244444847106934 and parameters: {'use_conv': True}. Best is trial 0 with value: 0.6244444847106934.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  5\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  5\n",
      "Best trial:\n",
      "  Value:  0.6244444847106934\n",
      "  Params: \n",
      "\tuse_conv: True\n"
     ]
    }
   ],
   "source": [
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    print(\"New trial\")\n",
    "\n",
    "    # Set up cross validation\n",
    "    n_splits: int = 3\n",
    "    fold = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    scores = [0]*n_splits\n",
    "\n",
    "    use_conv: bool = trial.suggest_categorical('use_conv', [True, False])\n",
    "\n",
    "    # Check if this trial has already been run before\n",
    "    for previous_trial in trial.study.trials:\n",
    "        if previous_trial.state == TrialState.COMPLETE and trial.params == previous_trial.params:\n",
    "            print(f\"Duplicated trial: {trial.params}, return {previous_trial.value}\")\n",
    "            return previous_trial.value\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(TRAIN_DATASET)))):\n",
    "        # train_idx and valid_idx are numpy arrays of indices of the training and validation sets for this fold respectively.\n",
    "        # They do not contain the actual data, but the indices of the data in the dataset.\n",
    "        # We can use these indices to create a subset of the dataset for this fold with torch.utils.data.Subset.\n",
    "        # Obviously, if an index is in the validation set, it will not be in the training set. You can\n",
    "        # check this by printing train_idx and valid_idx and check by yourself.\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}/{n_splits}\")\n",
    "\n",
    "        # Create subsets of the dataset for this fold\n",
    "        sub_train_data = torch.utils.data.Subset(TRAIN_DATASET, train_idx)\n",
    "        sub_valid_data = torch.utils.data.Subset(TRAIN_DATASET, valid_idx)\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        sub_train_loader = torch.utils.data.DataLoader(sub_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        sub_valid_loader = torch.utils.data.DataLoader(sub_valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        # Generate the model.\n",
    "        my_model: AdvancedNet = AdvancedNet(use_conv).to(DEVICE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training of the model.\n",
    "            # Put model in train mode\n",
    "            my_model.train()\n",
    "\n",
    "            # Set up optimizer\n",
    "            optimizer = torch.optim.SGD(my_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            # Set up loss function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            for X, y in sub_train_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "\n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "\n",
    "                # 3. Calculate and accumulate loss\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                # 4. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 5. Loss backward\n",
    "                loss.backward()\n",
    "\n",
    "                # 6. Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        # Put model in eval mode\n",
    "        my_model.eval()\n",
    "        \n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in sub_valid_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "                \n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "                \n",
    "                # 3. Compute accuracy\n",
    "                pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "                y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "                val_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        scores[fold_idx] = (val_acc / len(sub_valid_data)).cpu()\n",
    "        # bring it back otherwise, np.mean will not work\n",
    "        print(f\"Fold {fold_idx+1}/{n_splits} accuracy: {scores[fold_idx]}\")\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=1200, n_trials = 5) \n",
    "# - timeout=1200 -> stops after 20 minutes; \n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train with the hyperparameters that we found with Optuna. We will use the `study.best_params` attribute to get the best hyperparameters. You need to re-train on the whole training dataset!!! Otherwise, you will not get the best accuracy as you're leaving out some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: AdvancedNet = AdvancedNet(**study.best_params).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdvancedNet(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=262144, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_conv(loss_fn, optimizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and modified the trained model inplace.\n",
    "    \"\"\"\n",
    "    start_time_global = time.time()\n",
    "\n",
    "    # Put model in train mode\n",
    "    MODEL.train()\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Setup train loss and train accuracy values\n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for X, y in TRAIN_DATALOADER:\n",
    "            # 0. Reshape data to input to the network\n",
    "            pass  # we are happy with the shape BATCH_SIZE, 3, 64, 64\n",
    "\n",
    "            # 1. Move data to device\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # 2. Forward pass\n",
    "            y_pred = MODEL(X)\n",
    "\n",
    "            # 3. Calculate and accumulate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 4. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 5. Loss backward\n",
    "            loss.backward()\n",
    "\n",
    "            # 6. Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = y_pred.argmax(dim=1)\n",
    "            train_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / (BATCH_SIZE * len(TRAIN_DATALOADER))\n",
    "        train_acc = train_acc / (BATCH_SIZE * len(TRAIN_DATALOADER))\n",
    "        print(\n",
    "            f\"epoch {epoch+1}/{NUM_EPOCHS},\"\n",
    "            f\" train_loss = {train_loss:.2e},\"\n",
    "            f\" train_acc = {100*train_acc.item():.2f}%,\"\n",
    "            f\" time spent during this epoch = {time.time() - start_time_epoch:.2f}s,\"\n",
    "            f\" total time spent = {time.time() - start_time_global:.2f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, train_loss = 1.20e-01, train_acc = 53.11%, time spent during this epoch = 10.15s, total time spent = 10.15s\n",
      "epoch 2/15, train_loss = 1.05e-01, train_acc = 61.61%, time spent during this epoch = 9.07s, total time spent = 19.22s\n",
      "epoch 3/15, train_loss = 9.71e-02, train_acc = 65.87%, time spent during this epoch = 7.84s, total time spent = 27.06s\n",
      "epoch 4/15, train_loss = 8.97e-02, train_acc = 69.01%, time spent during this epoch = 9.61s, total time spent = 36.68s\n",
      "epoch 5/15, train_loss = 8.03e-02, train_acc = 73.41%, time spent during this epoch = 7.40s, total time spent = 44.08s\n",
      "epoch 6/15, train_loss = 6.97e-02, train_acc = 77.00%, time spent during this epoch = 7.09s, total time spent = 51.16s\n",
      "epoch 7/15, train_loss = 5.87e-02, train_acc = 81.14%, time spent during this epoch = 7.10s, total time spent = 58.26s\n",
      "epoch 8/15, train_loss = 4.51e-02, train_acc = 85.32%, time spent during this epoch = 8.12s, total time spent = 66.38s\n",
      "epoch 9/15, train_loss = 2.96e-02, train_acc = 90.90%, time spent during this epoch = 8.09s, total time spent = 74.47s\n",
      "epoch 10/15, train_loss = 2.03e-02, train_acc = 94.75%, time spent during this epoch = 7.03s, total time spent = 81.50s\n",
      "epoch 11/15, train_loss = 1.29e-02, train_acc = 96.49%, time spent during this epoch = 6.99s, total time spent = 88.50s\n",
      "epoch 12/15, train_loss = 1.18e-02, train_acc = 97.41%, time spent during this epoch = 7.39s, total time spent = 95.89s\n",
      "epoch 13/15, train_loss = 1.86e-03, train_acc = 99.78%, time spent during this epoch = 7.18s, total time spent = 103.07s\n",
      "epoch 14/15, train_loss = 6.32e-04, train_acc = 99.85%, time spent during this epoch = 6.89s, total time spent = 109.96s\n",
      "epoch 15/15, train_loss = 3.76e-04, train_acc = 99.85%, time spent during this epoch = 8.02s, total time spent = 117.98s\n"
     ]
    }
   ],
   "source": [
    "main_train_conv(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_our_model_conv() -> float:\n",
    "    # 0. Put model in eval mode\n",
    "    MODEL.eval()  # to remove stuff like dropout that's only going to be in the training part\n",
    "\n",
    "    # 1. Setup test accuracy value\n",
    "    test_acc: float = 0\n",
    "\n",
    "    # 2. Turn on inference context manager\n",
    "    with torch.no_grad():\n",
    "        # Loop through DataLoader batches\n",
    "        for X_test, y_test in TEST_DATALOADER:  # majuscule à X car c'est une \"matrice\", et y un entier\n",
    "            # a. Move data to device\n",
    "            X_test_flattened = X_test.to(DEVICE)  # no need to flatten here\n",
    "            y_test = y_test.to(DEVICE)\n",
    "\n",
    "            # b. Forward pass\n",
    "            model_output = MODEL(X_test_flattened)\n",
    "\n",
    "            # c. Calculate and accumulate accuracy\n",
    "            test_pred_label = model_output.argmax(dim=1)\n",
    "            test_acc += (test_pred_label == y_test).sum()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_acc = test_acc / (len(TEST_DATASET))\n",
    "    return test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.00%\n"
     ]
    }
   ],
   "source": [
    "print((f\"{100*test_our_model_conv():.2f}%\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely some sort of overfitting has happened here (look at the training accuracy!), but we did improve our accuracy (63.0% now against 55.67% earlier, and not far off what there was in the validation set (62.44%) on average (which makes sense))! This is not amazing though, that's why we should also optimise the learning rate (or the number of epochs), etc ... not just the architecture.\n",
    "\n",
    "Your turn now!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing learning rate and the number of channels after the first convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train the model with the best hyperparameters on the whole training set and test it on the testing set: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "689ffbb94fe8f58a5045b4f3f0726e738a118a8a590ae859861904a2cad8ac3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
