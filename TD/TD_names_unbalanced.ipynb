{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to redo the RNN guessing the name's nationality, with the full original dataset \"names\" instead of \"names_1000\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from unidecode import unidecode\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our alphabet\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
    "N_LETTERS = len(LETTERS)\n",
    "\n",
    "# turn a Unicode string to string of characters in our alphabet\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unidecode(s) if c in LETTERS)\n",
    "\n",
    "# turn a name into a <name_length x 1 x N_LETTERS>, or a tensor of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, N_LETTERS)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][LETTERS.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# create a custom dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        #read data\n",
    "        self.names = []\n",
    "        self.countries = []\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for line in lines:\n",
    "                self.names.append(unicodeToAscii(line))\n",
    "                self.countries.append(country)\n",
    "        self.n = len(self.names)\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, index):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[self.countries[index]]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, index):\n",
    "        return torch.tensor(self.country_to_idx[self.countries[index]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.names[index], self.countries[index], \\\n",
    "            nameToTensor(self.names[index]), self.countryID(index))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "dataset = NamesDataset('data/names/*.txt')\n",
    "# split data into train and test with random_split\n",
    "train_fraction = 0.8\n",
    "train_size = int(train_fraction*len(dataset))\n",
    "test_size = len(dataset)-int(train_fraction*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# create a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # batch_size 1 as names have different lengths\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network & Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for name, country, name_tensor, country_tensor in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on a couple of examples\n",
    "print(f'NAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_dataset):.2f}%')\n",
    "\n",
    "# plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy increased! (~75% vs ~60% last time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a couple of examples of our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Dupont\", \"Garcia\", \"Sato\", \"Dupr√®s\", \"Suzuki\", \"Wang\", \"Santos\", \"Yamamoto\"]\n",
    "for name in names:\n",
    "    name_tensor = nameToTensor(name)\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {rnn.outputToCountry(output)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost always english/russian that is predicted!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typical of a class unbalance, let's investigate the size of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cout each country in the dataset\n",
    "countr_count = {}\n",
    "total = 0\n",
    "for country in dataset.countries:\n",
    "    countr_count[country] = countr_count.get(country, 0) + 1\n",
    "    total += 1\n",
    "countr_count, total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the unbalanced learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st fixing idea: modify the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading names one by one, choose a country at random, then choose a name at random from this category.\n",
    "Do this using an iterable dataset (checkout the doc here: https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "import random\n",
    "\n",
    "# create a custom dataset\n",
    "class NamesIterableDataset(IterableDataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        #read data\n",
    "        self.names = dict()\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            self.names[country] = []\n",
    "            for line in lines:\n",
    "                self.names[country].append(unicodeToAscii(line))\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, country):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[country]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, country):\n",
    "        return torch.tensor(self.country_to_idx[country])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        # choose random country\n",
    "        country = random.choice(self.idx_to_country)\n",
    "        # choose random name from this country\n",
    "        name = random.choice(self.names[country])\n",
    "        # convert to tensors\n",
    "        name_tensor = nameToTensor(name)\n",
    "        countryID = self.countryID(country)\n",
    "        return (name, country, name_tensor, countryID)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 25000 # arbitrary, size of one epoch\n",
    "\n",
    "dataset = NamesIterableDataset('data/names/*.txt')\n",
    "# get a sample\n",
    "name, country, name_tensor, country_tensor = next(dataset)\n",
    "name, country, name_tensor.shape, country_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-define the RNN and re-train it with our new fancy iterable dataset.\n",
    "Print the final accuracy, and plot the confusion matrix (which should be closer to the identity matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# create a dataloader\n",
    "train_loader = DataLoader(dataset, batch_size=1) # batch_size 1 as names have different lengths\n",
    "test_loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for (name, country, name_tensor, country_tensor) in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_loader):.2f}%')\n",
    "\n",
    "# plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the accuracy went down again, but at least, our RNN isn't biased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training and testing set are the same, so the testing is less solid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd fixing idea: add a sampler to the dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse our previous dataset, and a sampler to the dataloader.\n",
    "Checkout the doc here: https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        #read data\n",
    "        self.names = []\n",
    "        self.countries = []\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for line in lines:\n",
    "                self.names.append(unicodeToAscii(line))\n",
    "                self.countries.append(country)\n",
    "        self.n = len(self.names)\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, index):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[self.countries[index]]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, index):\n",
    "        return torch.tensor(self.country_to_idx[self.countries[index]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.names[index], self.countries[index], \\\n",
    "            nameToTensor(self.names[index]), self.countryID(index))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "dataset = NamesDataset('data/names/*.txt')\n",
    "# split data into train and test with random_split\n",
    "train_fraction = 0.8\n",
    "train_size = int(train_fraction*len(dataset))\n",
    "test_size = len(dataset)-int(train_fraction*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# create a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "# create a random sampler that counterbalances the classes\n",
    "country_weights = torch.tensor([1/train_dataset.dataset.countries.count(country) for country in train_dataset.dataset.idx_to_country])\n",
    "#train\n",
    "train_items_weights = torch.tensor([country_weights[train_dataset.dataset.country_to_idx[country]] \\\n",
    "    for i,country in enumerate(train_dataset.dataset.countries) if i in train_dataset.indices])\n",
    "train_sampler = WeightedRandomSampler(train_items_weights, len(train_dataset), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=1)\n",
    "#test\n",
    "test_items_weights = torch.tensor([country_weights[test_dataset.dataset.country_to_idx[country]] \\\n",
    "    for i,country in enumerate(test_dataset.dataset.countries) if i in test_dataset.indices])\n",
    "test_sampler = WeightedRandomSampler(test_items_weights, len(test_dataset), replacement=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for name, country, name_tensor, country_tensor in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_dataset):.2f}%')\n",
    "\n",
    "# plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is higher, but the confusion matrix is more polarized; this technique only partially compensate for the class inbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd fixing idea: weight the loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse our previous dataset, dataloader, but modify the loss function to compensate the class sizes.\n",
    "Checkout the doc here: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        #read data\n",
    "        self.names = []\n",
    "        self.countries = []\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for line in lines:\n",
    "                self.names.append(unicodeToAscii(line))\n",
    "                self.countries.append(country)\n",
    "        self.n = len(self.names)\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, index):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[self.countries[index]]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, index):\n",
    "        return torch.tensor(self.country_to_idx[self.countries[index]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.names[index], self.countries[index], \\\n",
    "            nameToTensor(self.names[index]), self.countryID(index))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "dataset = NamesDataset('data/names/*.txt')\n",
    "# split data into train and test with random_split\n",
    "train_fraction = 0.8\n",
    "train_size = int(train_fraction*len(dataset))\n",
    "test_size = len(dataset)-int(train_fraction*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# create a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # batch_size 1 as names have different lengths\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "country_weights = torch.tensor([1/train_dataset.dataset.countries.count(country) for country in train_dataset.dataset.idx_to_country])\n",
    "criterion = nn.NLLLoss(country_weights)\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for name, country, name_tensor, country_tensor in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_dataset):.2f}%')\n",
    "\n",
    "# plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "689ffbb94fe8f58a5045b4f3f0726e738a118a8a590ae859861904a2cad8ac3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
