{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ovDJUER3rFK4"
      },
      "source": [
        "# TD 5c"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmJfTThvBnc"
      },
      "source": [
        "[Use pytorch for all questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R9k-Z90ivZfW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\__APP__\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN: Determinating Lastnames Origins"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal here is to build our first (basic) RNN network."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a datset composed of 18000 names, from 18 nationalities (1000 names from each country).\n",
        "We try to build a network to classify names to their correct nationality.\n",
        "We do that with a RNN, that \"reads\" each letter one by one."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to the dataset *name_1000*, containing 1000 names from 18 nationalities:\n",
        "https://drive.google.com/drive/folders/1qqyB_ZRMsz_7veqlKYnJH2kmYK6myV4Y?usp=share_link\n",
        "\n",
        "Start by downloading it and store it in your working directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some countries are using non-latin alphabet, we need the ASCII version.\n",
        "You can try the function `unidecode` from the `unidecode` mdule.\n",
        "\n",
        "Create a function that takes a name, and return its version using only letters from \n",
        "`LETTERS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'` (you can add some letters if you want, but the more you add, the more complex your network will become)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your function on `'Ślusàrski'`, `'François'`, `'北亰'`, `'Kožušček'`, and `'+-*/'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feeding letters to a network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A network can not, originally, process characters/letters; networks can only understand numbers, and list of numbers.\n",
        "We need to turn our characters to a vector. We could just take the binary byte representing the character in the ASCII. However, this would be very hard for the network to understand (`x: 1011000`,s `y:1011001` and `z: 1011010` will have very similar activations).\n",
        "Thus, we will use 'one-hot encoding' of our set of letters `LETTERS`. That is, we transform each letter to a tensor of size `<1xN_LETTERS>`, where all entries are zero except the one corresponding to the position of the letter, that we set to one.\n",
        "e.g.:\n",
        "- `a => [1, 0, 0, 0, ..., 0]`\n",
        "- `b => [0, 1, 0, 0, ..., 0]`\n",
        "- `c => [0, 0, 1, 0, ..., 0]`\n",
        "\n",
        "Define a `letterToTensor` function that perfoms this operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define a `nameToTensor` function that perfoms this operation for each letter in the name (resulting to a `<name_length x 1 x N_LETTERS>` tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a custom dataset:\n",
        "- in the `__init__`, read all files, and create a list of names and associated country\n",
        "- add a `countryID` method that turns a country its index\n",
        "- add a `countryTensor` method that turns a country to a one-hot encoded tensor\n",
        "- the `__getitem__` should return one piece of data in the form `(name, country, nameTensor, countryID)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split data into train and test with `torch.utils.data.dataset.random_split` (80% for training; 20% for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dataloader for the training and testing datasets; the `batch_size` must be 1, since different names can have different lengths (and therefore, different tensor size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the RNN with the following parameters:\n",
        "- input_size: number of input features\n",
        "- hidden_size: number of hidden units\n",
        "- output_size: number of output features\n",
        "- idx_to_country: list of countries\n",
        "\n",
        "The input is a one-hot vector of size `N_LETTERS`; the output is a one-hot vector of size `N_COUNTRIES = len(idx_to_country)` + a hidden state of size `hidden_size`.\n",
        "You can build the architecture you like, but one that is known to work is the following:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"../images/rnn.svg\" alt=\"rnn architecture\" style=\"width: 35em;\"/>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is, a simple dense layer that takes as input the concatenation of the hiden state and the current letter, and outputs both the new hidden state and a vector of likelyhood for each country. Adding a softmax layer to the countries likelyhood turns them to actual probabilities."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On top of the `__init__` and `forward` methods, define:\n",
        "- `init_hidden` a method that creates a zero hidden state (that we will use as a hidden state when sending the first letter)\n",
        "- `outputToCountry` to convert output probabilities to the corresponding country\n",
        "- `outputToID` to convert output probabilities to the corresponding country ID"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a network with 128 hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feeding the RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feed a single letter to the network (i.e. 1 step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feed a full word to the network (i.e. multiple steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the network; for each iteration of the training:\n",
        "- Create a zero initial hidden state\n",
        "- Feed each letter in and keep hidden state for next letter\n",
        "- Compute the loss\n",
        "- Back-propagate\n",
        "- Zero-out the gradients\n",
        "\n",
        "One configuration that is known to work (for the architecture described above):\n",
        "- Optimizer: Adam\n",
        "- Learning rate: `lr = 0.001`\n",
        "- Loss: Negative log likelihood (`NLLLoss`)\n",
        "- Epoch: No need for too many epoch (~5-10 is enough)\n",
        "\n",
        "NB: if you did not put a softmax, use `CrossEntropyLoss` instead of `NLLLoss`.\n",
        "\n",
        "*(Training takes ~3min on a modern laptop.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on a couple of names from the testing set, display the name, the prediction, and the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on the full test set:\n",
        "- Plot the confusion matrix\n",
        "- Compute the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy on the testing set is ~60%; this is much better than taking a random guess (which will have an accuracy of ~5.5%).\n",
        "\n",
        "You can do you own experiences to improve this result (add layers, test other layers such as LSTM or GRU, try some combinations).\n",
        "\n",
        "You can also change dataset (eg: word -> language; name -> gender; title -> newspaper; etc...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This RNN exercise was inspired from the following notebook of the official PyTorch documentation:\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "a61f982c8ae83496d3304782998ac96a47f5fcf9bfc174247e19a8162c5490e4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
