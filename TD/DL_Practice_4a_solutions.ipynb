{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate architectures and advanced PyTorch tools\n",
    "## TD 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are essentially going to use the same `Food101` ([credit where it's due](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)) data, the same object `ImageDataset`, the same `DataLoader`.\n",
    "\n",
    "The code below is mainly a copy of the code from the previous TD, except that global variables are now defined separately and everything is wrapped in different functions. This is to make it easier to train the same model with different hyperparameters and architectures, etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those that can use their GPUs, all the necessary `.to(device)` are already in the code.\n",
    "\n",
    "If, for some reason, you encounter this error: `OutOfMemoryError: CUDA out of memory.`. It means that your GPU does not have enough memory to run the model. You can try to reduce the batch size, or the number of neurons in the network, or the number of layers in the network, or the number of filters in the convolutional layers, etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "_ = torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the `flush` parameter to `True` for all `print()` statements in `Python` by overriding the built-in `print()` function using the `functools.partial()` method. An example of this is:\n",
    "\n",
    "```py\n",
    "from functools import partial\n",
    "print = partial(print, flush=True)\n",
    "```\n",
    "\n",
    "We will use this to make sure that the outputs are printed in the correct order and at the correct time (for more info, check [this link](https://www.includehelp.com/python/flush-parameter-in-python-with-print-function.aspx))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "print = partial(print, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "\n",
    "# Setup device-agnostic code\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 2e-2\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_and_dataloaders(\n",
    "    batch_size: int = 4\n",
    ") -> tuple[\n",
    "    datasets.ImageFolder, \n",
    "    datasets.ImageFolder, \n",
    "    DataLoader, \n",
    "    DataLoader\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the training and test datasets into data loaders.\n",
    "    \"\"\"\n",
    "    data_dir = pathlib.Path(\"data\")\n",
    "    train_dir = data_dir / \"Food-3\" / \"train\"\n",
    "    test_dir = data_dir / \"Food-3\" / \"test\"\n",
    "\n",
    "    data_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(64, 64)),  # Resize the images to 64x64*\n",
    "            transforms.ToTensor()  # Convert the images to tensors\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_data = datasets.ImageFolder(\n",
    "        root=train_dir,  # target folder of images\n",
    "        transform=data_transform,  # transforms to perform on data (images)\n",
    "        target_transform=None  # transforms to perform on labels (if necessary)\n",
    "    ) \n",
    "\n",
    "    test_data = datasets.ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=data_transform\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,  # how many samples per batch?\n",
    "        shuffle=True  # shuffle the data?\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    ) # don't usually need to shuffle testing data\n",
    "\n",
    "\n",
    "    return train_data, test_data, train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load dataloaders in global variables\n",
    "TRAIN_DATASET, TEST_DATASET, TRAIN_DATALOADER, TEST_DATALOADER = get_datasets_and_dataloaders(BATCH_SIZE)\n",
    "\n",
    "# We actually don't really need to return the datasets, but it's nice to have them for reference. If you don't,\n",
    "# you can just return the dataloaders and find the datasets by calling TRAIN_DATALOADER.dataset or TEST_DATALOADER.dataset:\n",
    "print(TRAIN_DATALOADER.dataset == TRAIN_DATASET)\n",
    "print(TEST_DATALOADER.dataset == TEST_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_units=200):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(64*64*3, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: Net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_our_model() -> float:\n",
    "    # 0. Put model in eval mode\n",
    "    MODEL.eval()  # to remove stuff like dropout that's only going to be in the training part\n",
    "\n",
    "    # 1. Setup test accuracy value\n",
    "    test_acc: float = 0\n",
    "\n",
    "    # 2. Turn on inference context manager\n",
    "    with torch.no_grad():\n",
    "        # Loop through DataLoader batches\n",
    "        for X_test, y_test in TEST_DATALOADER:  # majuscule Ã  X car c'est une \"matrice\", et y un entier\n",
    "            # a. Move data to device\n",
    "            X_test_flattened = X_test.view(-1, 64*64*3).to(DEVICE) \n",
    "            y_test = y_test.to(DEVICE)\n",
    "\n",
    "            # b. Forward pass\n",
    "            model_output = MODEL(X_test_flattened)\n",
    "\n",
    "            # c. Calculate and accumulate accuracy\n",
    "            test_pred_label = model_output.argmax(dim=1)\n",
    "            test_acc += (test_pred_label == y_test).sum()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_acc = test_acc / (len(TEST_DATASET))\n",
    "    return test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.00%\n"
     ]
    }
   ],
   "source": [
    "# Test our untrained model\n",
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 36.00% accuracy on the testing set without training and with the default hyperparameters if you used the same seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Why does it not work with ` X_test_flattened = X_test.view(BATCH_SIZE, 64*64*3).to(DEVICE)`?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(loss_fn, optimizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and modified the trained model inplace.\n",
    "    \"\"\"\n",
    "    start_time_global = time.time()\n",
    "\n",
    "    # Put model in train mode\n",
    "    MODEL.train()\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Setup train loss and train accuracy values\n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for X, y in TRAIN_DATALOADER:\n",
    "            # 0. Move data to device\n",
    "            X = X.view(-1, 64*64*3).to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = MODEL(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 4. Loss backward\n",
    "            loss.backward()\n",
    "\n",
    "            # 5. Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = y_pred.argmax(dim=1)\n",
    "            train_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / (len(TRAIN_DATASET))\n",
    "        train_acc = train_acc / (len(TRAIN_DATASET))\n",
    "        print(\n",
    "            f\"epoch {epoch+1}/{NUM_EPOCHS},\"\n",
    "            f\" train_loss = {train_loss:.2e},\"\n",
    "            f\" train_acc = {100*train_acc.item():.2f}%,\"\n",
    "            f\" time spent during this epoch = {time.time() - start_time_epoch:.2f}s,\"\n",
    "            f\" total time spent = {time.time() - start_time_global:.2f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, train_loss = 1.25e-01, train_acc = 49.81%, time spent during this epoch = 370.72s, total time spent = 370.72s\n",
      "epoch 2/15, train_loss = 1.16e-01, train_acc = 54.41%, time spent during this epoch = 79.50s, total time spent = 450.23s\n",
      "epoch 3/15, train_loss = 1.12e-01, train_acc = 57.63%, time spent during this epoch = 14.82s, total time spent = 465.05s\n",
      "epoch 4/15, train_loss = 1.09e-01, train_acc = 59.33%, time spent during this epoch = 10.66s, total time spent = 475.71s\n",
      "epoch 5/15, train_loss = 1.08e-01, train_acc = 60.41%, time spent during this epoch = 10.58s, total time spent = 486.29s\n",
      "epoch 6/15, train_loss = 1.05e-01, train_acc = 60.67%, time spent during this epoch = 10.62s, total time spent = 496.92s\n",
      "epoch 7/15, train_loss = 1.03e-01, train_acc = 61.30%, time spent during this epoch = 10.63s, total time spent = 507.55s\n",
      "epoch 8/15, train_loss = 1.01e-01, train_acc = 62.48%, time spent during this epoch = 10.57s, total time spent = 518.11s\n",
      "epoch 9/15, train_loss = 9.84e-02, train_acc = 65.07%, time spent during this epoch = 10.56s, total time spent = 528.68s\n",
      "epoch 10/15, train_loss = 9.64e-02, train_acc = 65.11%, time spent during this epoch = 10.57s, total time spent = 539.25s\n",
      "epoch 11/15, train_loss = 9.54e-02, train_acc = 64.78%, time spent during this epoch = 10.56s, total time spent = 549.81s\n",
      "epoch 12/15, train_loss = 9.20e-02, train_acc = 67.11%, time spent during this epoch = 10.58s, total time spent = 560.39s\n",
      "epoch 13/15, train_loss = 8.93e-02, train_acc = 67.93%, time spent during this epoch = 10.57s, total time spent = 570.97s\n",
      "epoch 14/15, train_loss = 8.67e-02, train_acc = 69.00%, time spent during this epoch = 10.59s, total time spent = 581.56s\n",
      "epoch 15/15, train_loss = 8.42e-02, train_acc = 69.85%, time spent during this epoch = 10.58s, total time spent = 592.14s\n"
     ]
    }
   ],
   "source": [
    "main_train(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.67%\n"
     ]
    }
   ],
   "source": [
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 55.67% accuracy on the testing set without training and with the default hyperparameters if you used the same seed. And we almost reached convergence (the loss is not decreasing that much anymore, and if you try to train for more epochs, you will see that the testing set accuracy will decrease). Note that we kind of cheated by using the testing set to set the number of epochs, we should instead use validation sets and cross validation techniques ... and we will (today)! No worries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible for `train_loss` to decrease whilst `train_acc` decreases at the same time? Look at what happens between epochs 10 and 11 here:\n",
    "\n",
    "```\n",
    "epoch 10/15, train_loss = 9.64e-02, train_acc = 65.11%, [...], total time spent = 121.83s\n",
    "epoch 11/15, train_loss = 9.54e-02, train_acc = 64.78%, [...], total time spent = 134.67s\n",
    "```\n",
    "\n",
    "Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to improve this accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install the Optuna package (`pip install optuna`) and import it at the beginning of your script. We should also import KFold from sklearn.model_selection. This is because we will use cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First easy task is to decide whether one should use a convolutional network or a dense network.\n",
    " \n",
    " We will do this together (choice between a convolutional and dense network), and then you'll have to implement optimization of the learning rate* and optimizer's choice on your own.\n",
    "\n",
    " \\* *Careful! Small learning rates are not always better, especially if you do not change the number of epochs. You should try to find the best learning rate for the number of epochs you chose, one that is not too big for your computer to handle.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNet(nn.Module):\n",
    "    def __init__(self, use_conv: bool, hidden_units: int = 200):\n",
    "        super(AdvancedNet, self).__init__()\n",
    "        self.use_conv: bool = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "            # output of this layer will be ((64+2*1-3)/1)+1 = 64. \n",
    "            # -> 64 channels of 64x64 images\n",
    "            self.fc1 = nn.Linear(64*64*64, hidden_units)  # flattening will be necessary to enter fc1\n",
    "            self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(3*64*64, hidden_units)\n",
    "            self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_conv:\n",
    "            x = nn.ReLU()(self.conv(x))\n",
    "            x = x.view(-1, 64*64*64)  # flattening is necessary, and, same as above,\n",
    "            # we need to use -1 and not BATCH_SIZE because the last batch might be smaller\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, you will need to define a new function that will be used as the objective function for Optuna's optimization. This function should take in the `trial` object from Optuna as an argument and use the `trial` object to define and sample the hyperparameters that you want to optimize. For example, you can use the `trial` object to sample a choice between a convolutional and dense network, and to sample the number of neurons for the chosen network. After training the model, we will need to return the final validation accuracy calculated with cross-validation* as the objective function value for Optuna to maximise.\n",
    "\n",
    " \\* We use cross-validation here (3-fold) because we want to use the testing set as little as possible. We will use the testing set only once, at the end, to get the final accuracy of the best model. But, cross-validation greatly increases the time required to run the algorithms, so we won't always use cross-validation to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    print(\"New trial\")\n",
    "\n",
    "    # Set up cross validation\n",
    "    n_splits: int = 3\n",
    "    fold = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    scores = [0]*n_splits\n",
    "\n",
    "    use_conv: bool = trial.suggest_categorical('use_conv', [True, False])\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(TRAIN_DATASET)))):\n",
    "        # train_idx and valid_idx are numpy arrays of indices of the training and validation sets for this fold respectively.\n",
    "        # They do not contain the actual data, but the indices of the data in the dataset.\n",
    "        # We can use these indices to create a subset of the dataset for this fold with torch.utils.data.Subset.\n",
    "        # Obviously, if an index is in the validation set, it will not be in the training set. You can\n",
    "        # check this by printing train_idx and valid_idx and check by yourself.\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}/{n_splits}\")\n",
    "\n",
    "        # Create subsets of the dataset for this fold\n",
    "        sub_train_data = torch.utils.data.Subset(TRAIN_DATASET, train_idx)\n",
    "        sub_valid_data = torch.utils.data.Subset(TRAIN_DATASET, valid_idx)\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        sub_train_loader = torch.utils.data.DataLoader(sub_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        sub_valid_loader = torch.utils.data.DataLoader(sub_valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        # Generate the model.\n",
    "        my_model: AdvancedNet = AdvancedNet(use_conv).to(DEVICE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training of the model.\n",
    "            # Put model in train mode\n",
    "            my_model.train()\n",
    "\n",
    "            # Set up optimizer\n",
    "            optimizer = torch.optim.SGD(my_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            # Set up loss function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            for X, y in sub_train_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "\n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "\n",
    "                # 3. Calculate and accumulate loss\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                # 4. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 5. Loss backward\n",
    "                loss.backward()\n",
    "\n",
    "                # 6. Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        # Put model in eval mode\n",
    "        my_model.eval()\n",
    "        \n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in sub_valid_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "                \n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "                \n",
    "                # 3. Compute accuracy\n",
    "                pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "                y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "                val_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        scores[fold_idx] = (val_acc / len(sub_valid_data)).cpu()\n",
    "        # bring it back otherwise, np.mean will not work\n",
    "        print(f\"Fold {fold_idx+1}/{n_splits} accuracy: {scores[fold_idx]}\")\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will need to call the `optuna.create_study()` function to create a new study, and use the `study.optimize()` function to run the optimization, passing the objective function that we defined earlier.\n",
    "\n",
    "You can find more information about how to use Optuna in the [Optuna documentation](https://optuna.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:10:17,113]\u001B[0m A new study created in memory with name: no-name-0b9a7089-56bf-4e9b-b479-062c95c4ab93\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6088889241218567\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.6177777647972107\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6322222352027893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:21:51,511]\u001B[0m Trial 0 finished with value: 0.6196296215057373 and parameters: {'use_conv': True}. Best is trial 0 with value: 0.6196296215057373.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6311111450195312\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.6200000047683716\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6355555653572083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:27:25,121]\u001B[0m Trial 1 finished with value: 0.6288889050483704 and parameters: {'use_conv': True}. Best is trial 1 with value: 0.6288889050483704.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  2\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  2\n",
      "Best trial:\n",
      "  Value:  0.6288889050483704\n",
      "  Params: \n",
      "\tuse_conv: True\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=1200, n_trials = 2) \n",
    "# - timeout=1200 -> stops after 20 minutes; \n",
    "# - n_trials = 2 -> here we only try two models, a dense or a convolutional model so\n",
    "#   we need to make it stop after having trained the two models otherwise it will continue to \n",
    "#   loop on those two models unless it reaches the 20 minutes mark*. In practice, you will give\n",
    "#   a lot of hyperparameters to optimize and you will want to run the optimization for a lot\n",
    "#   longer than 20 minutes. The timeout parameter is useful in those cases because you won't \n",
    "#   know how long it'll take.\n",
    "#   * e.g., https://i.imgur.com/bCzH1pm.png\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of you lot might have a problem: we've only allowed two trials but `Optuna` tried `False` then `False` or `True` then `True`. This is because `Optuna` doesn't check if it already has used the previous set of hyperparameters. To fix this, we can add the following code:\n",
    "\n",
    "```py\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "...\n",
    "\n",
    "for previous_trial in trial.study.trials:\n",
    "    if previous_trial.state == TrialState.COMPLETE and trial.params == previous_trial.params:\n",
    "        print(f\"Duplicated trial: {trial.params}, return {previous_trial.value}\")\n",
    "        return previous_trial.value\n",
    "```\n",
    "\n",
    "And set n_trials to 5 for example, that way it'll be very unlikely to have the same hyperparameters twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:27:25,175]\u001B[0m A new study created in memory with name: no-name-a65c2591-4c31-4f56-8ee2-dc1a55c75211\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6177777647972107\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.6044444441795349\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.5699999928474426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:32:51,664]\u001B[0m Trial 0 finished with value: 0.5974074006080627 and parameters: {'use_conv': False}. Best is trial 0 with value: 0.5974074006080627.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/3\n",
      "Fold 1/3 accuracy: 0.6100000143051147\n",
      "Fold 2/3\n",
      "Fold 2/3 accuracy: 0.643333375453949\n",
      "Fold 3/3\n",
      "Fold 3/3 accuracy: 0.6344444751739502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:38:25,012]\u001B[0m Trial 1 finished with value: 0.6292592883110046 and parameters: {'use_conv': True}. Best is trial 1 with value: 0.6292592883110046.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Duplicated trial: {'use_conv': True}, return 0.6292592883110046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:38:25,020]\u001B[0m Trial 2 finished with value: 0.6292592883110046 and parameters: {'use_conv': True}. Best is trial 1 with value: 0.6292592883110046.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Duplicated trial: {'use_conv': True}, return 0.6292592883110046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:38:25,027]\u001B[0m Trial 3 finished with value: 0.6292592883110046 and parameters: {'use_conv': True}. Best is trial 1 with value: 0.6292592883110046.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Duplicated trial: {'use_conv': True}, return 0.6292592883110046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:38:25,034]\u001B[0m Trial 4 finished with value: 0.6292592883110046 and parameters: {'use_conv': True}. Best is trial 1 with value: 0.6292592883110046.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  5\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  5\n",
      "Best trial:\n",
      "  Value:  0.6292592883110046\n",
      "  Params: \n",
      "\tuse_conv: True\n"
     ]
    }
   ],
   "source": [
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    print(\"New trial\")\n",
    "\n",
    "    # Set up cross validation\n",
    "    n_splits: int = 3\n",
    "    fold = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    scores = [0]*n_splits\n",
    "\n",
    "    use_conv: bool = trial.suggest_categorical('use_conv', [True, False])\n",
    "\n",
    "    # Check if this trial has already been run before\n",
    "    for previous_trial in trial.study.trials:\n",
    "        if previous_trial.state == TrialState.COMPLETE and trial.params == previous_trial.params:\n",
    "            print(f\"Duplicated trial: {trial.params}, return {previous_trial.value}\")\n",
    "            return previous_trial.value\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(TRAIN_DATASET)))):\n",
    "        # train_idx and valid_idx are numpy arrays of indices of the training and validation sets for this fold respectively.\n",
    "        # They do not contain the actual data, but the indices of the data in the dataset.\n",
    "        # We can use these indices to create a subset of the dataset for this fold with torch.utils.data.Subset.\n",
    "        # Obviously, if an index is in the validation set, it will not be in the training set. You can\n",
    "        # check this by printing train_idx and valid_idx and check by yourself.\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}/{n_splits}\")\n",
    "\n",
    "        # Create subsets of the dataset for this fold\n",
    "        sub_train_data = torch.utils.data.Subset(TRAIN_DATASET, train_idx)\n",
    "        sub_valid_data = torch.utils.data.Subset(TRAIN_DATASET, valid_idx)\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        sub_train_loader = torch.utils.data.DataLoader(sub_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        sub_valid_loader = torch.utils.data.DataLoader(sub_valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        # Generate the model.\n",
    "        my_model: AdvancedNet = AdvancedNet(use_conv).to(DEVICE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training of the model.\n",
    "            # Put model in train mode\n",
    "            my_model.train()\n",
    "\n",
    "            # Set up optimizer\n",
    "            optimizer = torch.optim.SGD(my_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            # Set up loss function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            for X, y in sub_train_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "\n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "\n",
    "                # 3. Calculate and accumulate loss\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                # 4. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 5. Loss backward\n",
    "                loss.backward()\n",
    "\n",
    "                # 6. Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        # Put model in eval mode\n",
    "        my_model.eval()\n",
    "        \n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in sub_valid_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "                \n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "                \n",
    "                # 3. Compute accuracy\n",
    "                pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "                y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "                val_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        scores[fold_idx] = (val_acc / len(sub_valid_data)).cpu()\n",
    "        # bring it back otherwise, np.mean will not work\n",
    "        print(f\"Fold {fold_idx+1}/{n_splits} accuracy: {scores[fold_idx]}\")\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=1200, n_trials = 5) \n",
    "# - timeout=1200 -> stops after 20 minutes; \n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train with the hyperparameters that we found with Optuna. We will use the `study.best_params` attribute to get the best hyperparameters. You need to re-train on the whole training dataset!!! Otherwise, you will not get the best accuracy as you're leaving out some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: AdvancedNet = AdvancedNet(**study.best_params).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdvancedNet(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=262144, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_conv(loss_fn, optimizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and modified the trained model inplace.\n",
    "    \"\"\"\n",
    "    start_time_global = time.time()\n",
    "\n",
    "    # Put model in train mode\n",
    "    MODEL.train()\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Setup train loss and train accuracy values\n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for X, y in TRAIN_DATALOADER:\n",
    "            # 0. Reshape data to input to the network\n",
    "            pass  # we are happy with the shape BATCH_SIZE, 3, 64, 64\n",
    "\n",
    "            # 1. Move data to device\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # 2. Forward pass\n",
    "            y_pred = MODEL(X)\n",
    "\n",
    "            # 3. Calculate and accumulate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 4. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 5. Loss backward\n",
    "            loss.backward()\n",
    "\n",
    "            # 6. Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = y_pred.argmax(dim=1)\n",
    "            train_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / (BATCH_SIZE * len(TRAIN_DATALOADER))\n",
    "        train_acc = train_acc / (BATCH_SIZE * len(TRAIN_DATALOADER))\n",
    "        print(\n",
    "            f\"epoch {epoch+1}/{NUM_EPOCHS},\"\n",
    "            f\" train_loss = {train_loss:.2e},\"\n",
    "            f\" train_acc = {100*train_acc.item():.2f}%,\"\n",
    "            f\" time spent during this epoch = {time.time() - start_time_epoch:.2f}s,\"\n",
    "            f\" total time spent = {time.time() - start_time_global:.2f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, train_loss = 1.20e-01, train_acc = 53.25%, time spent during this epoch = 11.29s, total time spent = 11.29s\n",
      "epoch 2/15, train_loss = 1.05e-01, train_acc = 62.17%, time spent during this epoch = 11.27s, total time spent = 22.56s\n",
      "epoch 3/15, train_loss = 9.74e-02, train_acc = 66.05%, time spent during this epoch = 11.29s, total time spent = 33.85s\n",
      "epoch 4/15, train_loss = 9.00e-02, train_acc = 68.16%, time spent during this epoch = 11.26s, total time spent = 45.11s\n",
      "epoch 5/15, train_loss = 8.10e-02, train_acc = 72.82%, time spent during this epoch = 11.27s, total time spent = 56.39s\n",
      "epoch 6/15, train_loss = 7.09e-02, train_acc = 76.48%, time spent during this epoch = 11.28s, total time spent = 67.66s\n",
      "epoch 7/15, train_loss = 5.87e-02, train_acc = 80.95%, time spent during this epoch = 11.37s, total time spent = 79.03s\n",
      "epoch 8/15, train_loss = 4.42e-02, train_acc = 86.50%, time spent during this epoch = 11.32s, total time spent = 90.35s\n",
      "epoch 9/15, train_loss = 3.15e-02, train_acc = 90.79%, time spent during this epoch = 11.33s, total time spent = 101.69s\n",
      "epoch 10/15, train_loss = 2.14e-02, train_acc = 94.45%, time spent during this epoch = 11.33s, total time spent = 113.02s\n",
      "epoch 11/15, train_loss = 1.11e-02, train_acc = 97.34%, time spent during this epoch = 11.33s, total time spent = 124.35s\n",
      "epoch 12/15, train_loss = 1.03e-02, train_acc = 97.15%, time spent during this epoch = 11.29s, total time spent = 135.64s\n",
      "epoch 13/15, train_loss = 4.92e-03, train_acc = 98.74%, time spent during this epoch = 11.28s, total time spent = 146.93s\n",
      "epoch 14/15, train_loss = 9.68e-04, train_acc = 99.85%, time spent during this epoch = 11.30s, total time spent = 158.23s\n",
      "epoch 15/15, train_loss = 4.14e-04, train_acc = 99.85%, time spent during this epoch = 11.30s, total time spent = 169.54s\n"
     ]
    }
   ],
   "source": [
    "main_train_conv(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_our_model_conv() -> float:\n",
    "    # 0. Put model in eval mode\n",
    "    MODEL.eval()  # to remove stuff like dropout that's only going to be in the training part\n",
    "\n",
    "    # 1. Setup test accuracy value\n",
    "    test_acc: float = 0\n",
    "\n",
    "    # 2. Turn on inference context manager\n",
    "    with torch.no_grad():\n",
    "        # Loop through DataLoader batches\n",
    "        for X_test, y_test in TEST_DATALOADER:  # majuscule Ã  X car c'est une \"matrice\", et y un entier\n",
    "            # a. Move data to device\n",
    "            X_test_flattened = X_test.to(DEVICE)  # no need to flatten here\n",
    "            y_test = y_test.to(DEVICE)\n",
    "\n",
    "            # b. Forward pass\n",
    "            model_output = MODEL(X_test_flattened)\n",
    "\n",
    "            # c. Calculate and accumulate accuracy\n",
    "            test_pred_label = model_output.argmax(dim=1)\n",
    "            test_acc += (test_pred_label == y_test).sum()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_acc = test_acc / (len(TEST_DATASET))\n",
    "    return test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.33%\n"
     ]
    }
   ],
   "source": [
    "print((f\"{100*test_our_model_conv():.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely some sort of overfitting has happened here (look at the training accuracy!), but we did improve our accuracy (62.33% now against 55.67% earlier, and not far off what there was in the validation set (62.92%) on average (which makes sense))! This is not amazing though, that's why we should also optimise the learning rate (or the number of epochs), etc ... not just the architecture.\n",
    "\n",
    "Your turn now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing learning rate and the number of channels after the first convolution layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 22:55:38,580]\u001B[0m A new study created in memory with name: no-name-599dea74-db32-4ef3-9189-ba88c961bcad\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5999999642372131\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6037036776542664\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5592592358589172\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5870370268821716\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5833333134651184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 23:07:08,969]\u001B[0m Trial 0 finished with value: 0.5866666436195374 and parameters: {'use_conv': True, 'out_channels': 31, 'learning_rate': 0.00027216975756670815}. Best is trial 0 with value: 0.5866666436195374.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6092592477798462\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.585185170173645\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5814814567565918\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5796296000480652\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5833333134651184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 23:17:50,002]\u001B[0m Trial 1 finished with value: 0.5877777338027954 and parameters: {'use_conv': False, 'learning_rate': 0.012377683289352596}. Best is trial 1 with value: 0.5877777338027954.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.605555534362793\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6722221970558167\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6222221851348877\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5870370268821716\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6611111164093018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 23:28:42,554]\u001B[0m Trial 2 finished with value: 0.6296296119689941 and parameters: {'use_conv': True, 'out_channels': 36, 'learning_rate': 0.007463640154675805}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6092592477798462\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5777777433395386\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.555555522441864\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5537036657333374\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.555555522441864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 23:39:26,019]\u001B[0m Trial 3 finished with value: 0.5703703165054321 and parameters: {'use_conv': False, 'learning_rate': 0.0001308924002348598}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5259259343147278\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5370370149612427\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5407407283782959\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5277777910232544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-24 23:50:17,523]\u001B[0m Trial 4 finished with value: 0.5262962579727173 and parameters: {'use_conv': True, 'out_channels': 15, 'learning_rate': 0.08267596829405642}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.31111109256744385\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.3333333134651184\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.32777777314186096\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.3185185194015503\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.34074074029922485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 00:02:11,550]\u001B[0m Trial 5 finished with value: 0.32629626989364624 and parameters: {'use_conv': False, 'learning_rate': 0.08458397978025306}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6222221851348877\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6240740418434143\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5925925970077515\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5944444537162781\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5962963104248047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 00:21:16,374]\u001B[0m Trial 6 finished with value: 0.6059259176254272 and parameters: {'use_conv': True, 'out_channels': 55, 'learning_rate': 0.000455960667565156}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6092592477798462\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5962963104248047\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5962963104248047\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5592592358589172\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5629629492759705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 00:39:23,061]\u001B[0m Trial 7 finished with value: 0.5848148465156555 and parameters: {'use_conv': False, 'learning_rate': 0.010172336852831261}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5796296000480652\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.614814817905426\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.585185170173645\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6111111044883728\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.585185170173645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 00:50:09,881]\u001B[0m Trial 8 finished with value: 0.5951851606369019 and parameters: {'use_conv': False, 'learning_rate': 0.012866546188185862}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5962963104248047\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5870370268821716\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5592592358589172\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5462962985038757\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5629629492759705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 01:01:28,176]\u001B[0m Trial 9 finished with value: 0.5703703761100769 and parameters: {'use_conv': False, 'learning_rate': 0.0001263659182092633}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5925925970077515\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5703703761100769\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5629629492759705\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.575925886631012\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5629629492759705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 01:12:48,010]\u001B[0m Trial 10 finished with value: 0.5729629397392273 and parameters: {'use_conv': True, 'out_channels': 50, 'learning_rate': 4.2841416432543415e-05}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6222221851348877\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6129629611968994\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6185185313224792\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.585185170173645\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6222221851348877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 01:23:52,100]\u001B[0m Trial 11 finished with value: 0.6122222542762756 and parameters: {'use_conv': True, 'out_channels': 62, 'learning_rate': 0.0011904814964542464}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6166666746139526\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6444444060325623\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6333333253860474\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5907407402992249\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6314814686775208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 01:36:21,251]\u001B[0m Trial 12 finished with value: 0.6233333349227905 and parameters: {'use_conv': True, 'out_channels': 36, 'learning_rate': 0.0025089581111183232}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6240740418434143\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6333333253860474\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5629629492759705\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5814814567565918\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6203703880310059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 01:47:12,745]\u001B[0m Trial 13 finished with value: 0.6044444441795349 and parameters: {'use_conv': True, 'out_channels': 33, 'learning_rate': 0.0019809363071112557}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6074073910713196\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6277777552604675\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6185185313224792\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6129629611968994\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6518518328666687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 02:24:21,723]\u001B[0m Trial 14 finished with value: 0.6237037181854248 and parameters: {'use_conv': True, 'out_channels': 41, 'learning_rate': 0.002763394800201883}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5888888835906982\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5888888835906982\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5388888716697693\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5222222208976746\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5333333015441895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 02:36:07,150]\u001B[0m Trial 15 finished with value: 0.554444432258606 and parameters: {'use_conv': True, 'out_channels': 41, 'learning_rate': 1.1773911707213765e-05}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6074073910713196\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6555555462837219\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.614814817905426\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5722222328186035\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6537036895751953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 02:47:29,351]\u001B[0m Trial 16 finished with value: 0.6207407712936401 and parameters: {'use_conv': True, 'out_channels': 15, 'learning_rate': 0.00484976276163504}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6166666746139526\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6407407522201538\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5981481671333313\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5740740895271301\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6203703880310059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 02:58:41,385]\u001B[0m Trial 17 finished with value: 0.6100000143051147 and parameters: {'use_conv': True, 'out_channels': 24, 'learning_rate': 0.0009586359059350767}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5999999642372131\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5777777433395386\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6074073910713196\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6166666746139526\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6259258985519409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 03:09:45,788]\u001B[0m Trial 18 finished with value: 0.605555534362793 and parameters: {'use_conv': True, 'out_channels': 46, 'learning_rate': 0.004745910145905113}. Best is trial 2 with value: 0.6296296119689941.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5981481671333313\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.664814829826355\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6314814686775208\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6222221851348877\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6574074029922485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 03:21:03,870]\u001B[0m Trial 19 finished with value: 0.6348148584365845 and parameters: {'use_conv': True, 'out_channels': 25, 'learning_rate': 0.022887831450837906}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6018518209457397\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6129629611968994\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5962963104248047\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5981481671333313\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5999999642372131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 03:32:22,557]\u001B[0m Trial 20 finished with value: 0.6018518209457397 and parameters: {'use_conv': True, 'out_channels': 5, 'learning_rate': 0.027377406551207698}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5925925970077515\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6481481194496155\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6462962627410889\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5685185194015503\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6314814686775208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 03:43:47,703]\u001B[0m Trial 21 finished with value: 0.6174073815345764 and parameters: {'use_conv': True, 'out_channels': 24, 'learning_rate': 0.03983890122097303}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6111111044883728\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.635185182094574\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6185185313224792\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6074073910713196\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6370370388031006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 03:55:02,374]\u001B[0m Trial 22 finished with value: 0.6218518018722534 and parameters: {'use_conv': True, 'out_channels': 41, 'learning_rate': 0.005166970386374458}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6203703880310059\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6666666269302368\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6296296119689941\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5962963104248047\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6388888955116272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 04:05:58,448]\u001B[0m Trial 23 finished with value: 0.6303703188896179 and parameters: {'use_conv': True, 'out_channels': 26, 'learning_rate': 0.027713339707092735}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5981481671333313\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6370370388031006\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6018518209457397\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.614814817905426\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6592592597007751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 04:16:54,966]\u001B[0m Trial 24 finished with value: 0.6222222447395325 and parameters: {'use_conv': True, 'out_channels': 25, 'learning_rate': 0.040406319302466874}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.614814817905426\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6259258985519409\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6259258985519409\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5574073791503906\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5999999642372131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 04:27:51,324]\u001B[0m Trial 25 finished with value: 0.6048148274421692 and parameters: {'use_conv': True, 'out_channels': 14, 'learning_rate': 0.02285730224886605}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6092592477798462\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6296296119689941\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6185185313224792\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5981481671333313\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6666666269302368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 04:38:47,262]\u001B[0m Trial 26 finished with value: 0.6244443655014038 and parameters: {'use_conv': True, 'out_channels': 30, 'learning_rate': 0.014457960417158107}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.575925886631012\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5944444537162781\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5925925970077515\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.555555522441864\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6129629611968994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 04:49:40,838]\u001B[0m Trial 27 finished with value: 0.5862962603569031 and parameters: {'use_conv': True, 'out_channels': 20, 'learning_rate': 0.049981134753738526}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6203703880310059\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.605555534362793\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5962963104248047\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5777777433395386\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6092592477798462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 05:00:32,457]\u001B[0m Trial 28 finished with value: 0.6018518209457397 and parameters: {'use_conv': True, 'out_channels': 6, 'learning_rate': 0.008072868356308301}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6296296119689941\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6444444060325623\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6111111044883728\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6166666746139526\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6555555462837219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 05:11:24,390]\u001B[0m Trial 29 finished with value: 0.6314815282821655 and parameters: {'use_conv': True, 'out_channels': 27, 'learning_rate': 0.023370030204294713}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5962963104248047\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6481481194496155\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6444444060325623\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5777777433395386\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6611111164093018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 05:22:15,619]\u001B[0m Trial 30 finished with value: 0.6255555748939514 and parameters: {'use_conv': True, 'out_channels': 20, 'learning_rate': 0.02192934902502196}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6240740418434143\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6444444060325623\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6425926089286804\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6111111044883728\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6425926089286804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 05:33:06,368]\u001B[0m Trial 31 finished with value: 0.6329630017280579 and parameters: {'use_conv': True, 'out_channels': 28, 'learning_rate': 0.02025520278264623}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6314814686775208\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6388888955116272\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6333333253860474\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5796296000480652\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6407407522201538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 05:43:57,089]\u001B[0m Trial 32 finished with value: 0.6248148083686829 and parameters: {'use_conv': True, 'out_channels': 28, 'learning_rate': 0.016303241420515844}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5703703761100769\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5703703761100769\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5777777433395386\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5425925850868225\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6092592477798462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 05:54:47,415]\u001B[0m Trial 33 finished with value: 0.5740740895271301 and parameters: {'use_conv': True, 'out_channels': 19, 'learning_rate': 0.05310173837255445}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5740740895271301\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6462962627410889\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5481481552124023\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5444444417953491\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5814814567565918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 06:05:26,634]\u001B[0m Trial 34 finished with value: 0.5788888931274414 and parameters: {'use_conv': False, 'learning_rate': 0.026845609386660033}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5240740776062012\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5018518567085266\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.49444442987442017\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5185185074806213\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5351851582527161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 06:16:17,866]\u001B[0m Trial 35 finished with value: 0.5148147344589233 and parameters: {'use_conv': True, 'out_channels': 27, 'learning_rate': 0.08641576602056762}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.575925886631012\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6574074029922485\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6129629611968994\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5944444537162781\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6111111044883728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 06:27:09,252]\u001B[0m Trial 36 finished with value: 0.610370397567749 and parameters: {'use_conv': True, 'out_channels': 35, 'learning_rate': 0.011489379054081298}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6203703880310059\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5796296000480652\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6074073910713196\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6092592477798462\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.585185170173645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 06:37:49,960]\u001B[0m Trial 37 finished with value: 0.6003702878952026 and parameters: {'use_conv': False, 'learning_rate': 0.007849239120912484}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5870370268821716\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5740740895271301\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6092592477798462\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5814814567565918\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5722222328186035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 06:48:41,505]\u001B[0m Trial 38 finished with value: 0.5848148465156555 and parameters: {'use_conv': True, 'out_channels': 31, 'learning_rate': 0.0659425965101501}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5907407402992249\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.635185182094574\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6240740418434143\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5907407402992249\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6555555462837219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 06:59:33,297]\u001B[0m Trial 39 finished with value: 0.619259238243103 and parameters: {'use_conv': True, 'out_channels': 22, 'learning_rate': 0.0351018937330664}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5833333134651184\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6333333253860474\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5666666626930237\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5925925970077515\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6074073910713196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 07:10:14,405]\u001B[0m Trial 40 finished with value: 0.596666693687439 and parameters: {'use_conv': False, 'learning_rate': 0.018836004308406654}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6018518209457397\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6370370388031006\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6111111044883728\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6037036776542664\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6462962627410889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 07:21:07,299]\u001B[0m Trial 41 finished with value: 0.6200000047683716 and parameters: {'use_conv': True, 'out_channels': 36, 'learning_rate': 0.0343526482690169}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5833333134651184\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6555555462837219\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6259258985519409\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.6425926089286804\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6574074029922485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 07:31:59,397]\u001B[0m Trial 42 finished with value: 0.6329630017280579 and parameters: {'use_conv': True, 'out_channels': 31, 'learning_rate': 0.016678893948021103}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5148147940635681\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5351851582527161\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5074074268341064\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.49444442987442017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 07:42:50,789]\u001B[0m Trial 43 finished with value: 0.5103703737258911 and parameters: {'use_conv': True, 'out_channels': 29, 'learning_rate': 0.09079387961250182}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6314814686775208\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5907407402992249\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6074073910713196\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5888888835906982\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6388888955116272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 07:53:41,561]\u001B[0m Trial 44 finished with value: 0.6114814877510071 and parameters: {'use_conv': True, 'out_channels': 12, 'learning_rate': 0.012863339725357914}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6111111044883728\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6592592597007751\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6129629611968994\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5962963104248047\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6777777671813965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 08:04:33,571]\u001B[0m Trial 45 finished with value: 0.6314814686775208 and parameters: {'use_conv': True, 'out_channels': 32, 'learning_rate': 0.020164333300676126}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6240740418434143\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.605555534362793\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5962963104248047\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5037037134170532\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5740740895271301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 08:15:14,238]\u001B[0m Trial 46 finished with value: 0.5807406902313232 and parameters: {'use_conv': False, 'learning_rate': 0.017465756788065796}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.6129629611968994\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.614814817905426\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5944444537162781\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5685185194015503\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.5425925850868225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 08:26:06,444]\u001B[0m Trial 47 finished with value: 0.5866667032241821 and parameters: {'use_conv': True, 'out_channels': 39, 'learning_rate': 0.059138658313891215}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.5925925970077515\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.6518518328666687\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.605555534362793\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.614814817905426\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6333333253860474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 08:36:58,637]\u001B[0m Trial 48 finished with value: 0.6196295619010925 and parameters: {'use_conv': True, 'out_channels': 32, 'learning_rate': 0.009877135285585288}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.4981481432914734\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.5629629492759705\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.5351851582527161\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5388888716697693\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.520370364189148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 08:47:51,073]\u001B[0m Trial 49 finished with value: 0.5311111211776733 and parameters: {'use_conv': True, 'out_channels': 47, 'learning_rate': 0.09997980532176738}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New trial\n",
      "Fold 1/5\n",
      "Fold 1/5 accuracy: 0.614814817905426\n",
      "Fold 2/5\n",
      "Fold 2/5 accuracy: 0.664814829826355\n",
      "Fold 3/5\n",
      "Fold 3/5 accuracy: 0.6277777552604675\n",
      "Fold 4/5\n",
      "Fold 4/5 accuracy: 0.5944444537162781\n",
      "Fold 5/5\n",
      "Fold 5/5 accuracy: 0.6129629611968994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-25 08:58:42,823]\u001B[0m Trial 50 finished with value: 0.622963011264801 and parameters: {'use_conv': True, 'out_channels': 32, 'learning_rate': 0.015853111650765268}. Best is trial 19 with value: 0.6348148584365845.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  51\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  51\n",
      "Best trial:\n",
      "  Value:  0.6348148584365845\n",
      "  Params: \n",
      "\tuse_conv: True\n",
      "\tout_channels: 25\n",
      "\tlearning_rate: 0.022887831450837906\n"
     ]
    }
   ],
   "source": [
    "class AdvancedNet2(nn.Module):\n",
    "    def __init__(self, use_conv: bool, out_channels: int, hidden_units: int = 200):\n",
    "        super(AdvancedNet2, self).__init__()\n",
    "        self.use_conv: bool = use_conv\n",
    "        self.out_channels: int = out_channels\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(3, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            # output of this layer will be ((64+2*1-3)/1)+1 = 64. \n",
    "            # -> 64 channels of 64x64 images\n",
    "            self.fc1 = nn.Linear(out_channels*64*64, hidden_units)  # flattening will be necessary to enter fc1\n",
    "            self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(3*64*64, hidden_units)\n",
    "            self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_conv:\n",
    "            x = nn.ReLU()(self.conv(x))\n",
    "            x = x.view(-1, self.out_channels*64*64)  # flattening is necessary, and, same as above,\n",
    "            # we need to use -1 and not BATCH_SIZE because the last batch might be smaller\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    print(\"New trial\")\n",
    "\n",
    "    # Set up cross validation\n",
    "    n_splits: int = 5\n",
    "    fold = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    scores = [0]*n_splits\n",
    "\n",
    "    use_conv: bool = trial.suggest_categorical('use_conv', [True, False])\n",
    "    if use_conv:\n",
    "        out_channels: int = trial.suggest_int('out_channels', 3, 64)\n",
    "    else:\n",
    "        out_channels: int = 0\n",
    "    learning_rate_to_optimise: float = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "\n",
    "    # Check if this trial has already been run before\n",
    "    for previous_trial in trial.study.trials:\n",
    "        if previous_trial.state == TrialState.COMPLETE and trial.params == previous_trial.params:\n",
    "            print(f\"Duplicated trial: {trial.params}, return {previous_trial.value}\")\n",
    "            return previous_trial.value\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(TRAIN_DATASET)))):\n",
    "        # train_idx and valid_idx are numpy arrays of indices of the training and validation sets for this fold respectively.\n",
    "        # They do not contain the actual data, but the indices of the data in the dataset.\n",
    "        # We can use these indices to create a subset of the dataset for this fold with torch.utils.data.Subset.\n",
    "        # Obviously, if an index is in the validation set, it will not be in the training set. You can\n",
    "        # check this by printing train_idx and valid_idx and check by yourself.\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1}/{n_splits}\")\n",
    "\n",
    "        # Create subsets of the dataset for this fold\n",
    "        sub_train_data = torch.utils.data.Subset(TRAIN_DATASET, train_idx)\n",
    "        sub_valid_data = torch.utils.data.Subset(TRAIN_DATASET, valid_idx)\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        sub_train_loader = torch.utils.data.DataLoader(sub_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        sub_valid_loader = torch.utils.data.DataLoader(sub_valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        # Generate the model.\n",
    "        my_model: AdvancedNet2 = AdvancedNet2(use_conv, out_channels).to(DEVICE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training of the model.\n",
    "            # Put model in train mode\n",
    "            my_model.train()\n",
    "\n",
    "            # Set up optimizer\n",
    "            optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate_to_optimise)\n",
    "\n",
    "            # Set up loss function\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            for X, y in sub_train_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "\n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "\n",
    "                # 3. Calculate and accumulate loss\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                # 4. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 5. Loss backward\n",
    "                loss.backward()\n",
    "\n",
    "                # 6. Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        # Put model in eval mode\n",
    "        my_model.eval()\n",
    "        \n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in sub_valid_loader:\n",
    "                # 0. Reshape data to input to the network\n",
    "                if use_conv:\n",
    "                    pass\n",
    "                else:\n",
    "                    X = X.view(-1, 64*64*3)\n",
    "                \n",
    "                # 1. Move data to device\n",
    "                X = X.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                # 2. Forward pass\n",
    "                y_pred = my_model(X)\n",
    "                \n",
    "                # 3. Compute accuracy\n",
    "                pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "                y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "                val_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        scores[fold_idx] = (val_acc / len(sub_valid_data)).cpu()\n",
    "        # bring it back otherwise, np.mean will not work\n",
    "        print(f\"Fold {fold_idx+1}/{n_splits} accuracy: {scores[fold_idx]}\")\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=36000, n_trials = 500) \n",
    "# - timeout=3600 -> stops after 10 hours or 500 trials, whichever comes first; \n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train the model with the best hyperparameters on the whole training set and test it on the testing set: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: AdvancedNet2 = AdvancedNet2(out_channels=25, use_conv=True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdvancedNet2(\n",
       "  (conv): Conv2d(3, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=102400, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/15, train_loss = 1.20e-01, train_acc = 53.92%, time spent during this epoch = 10.96s, total time spent = 10.96s\n",
      "epoch 2/15, train_loss = 1.07e-01, train_acc = 60.80%, time spent during this epoch = 10.93s, total time spent = 21.90s\n",
      "epoch 3/15, train_loss = 1.00e-01, train_acc = 64.20%, time spent during this epoch = 10.93s, total time spent = 32.83s\n",
      "epoch 4/15, train_loss = 9.43e-02, train_acc = 66.72%, time spent during this epoch = 10.93s, total time spent = 43.77s\n",
      "epoch 5/15, train_loss = 8.47e-02, train_acc = 71.30%, time spent during this epoch = 10.94s, total time spent = 54.71s\n",
      "epoch 6/15, train_loss = 7.41e-02, train_acc = 75.26%, time spent during this epoch = 10.94s, total time spent = 65.66s\n",
      "epoch 7/15, train_loss = 6.42e-02, train_acc = 78.51%, time spent during this epoch = 10.94s, total time spent = 76.60s\n",
      "epoch 8/15, train_loss = 5.07e-02, train_acc = 83.28%, time spent during this epoch = 10.95s, total time spent = 87.55s\n",
      "epoch 9/15, train_loss = 3.73e-02, train_acc = 89.02%, time spent during this epoch = 10.95s, total time spent = 98.50s\n",
      "epoch 10/15, train_loss = 2.40e-02, train_acc = 93.31%, time spent during this epoch = 11.04s, total time spent = 109.55s\n",
      "epoch 11/15, train_loss = 1.31e-02, train_acc = 96.63%, time spent during this epoch = 10.95s, total time spent = 120.50s\n",
      "epoch 12/15, train_loss = 9.03e-03, train_acc = 97.52%, time spent during this epoch = 10.94s, total time spent = 131.44s\n",
      "epoch 13/15, train_loss = 1.69e-03, train_acc = 99.74%, time spent during this epoch = 10.95s, total time spent = 142.39s\n",
      "epoch 14/15, train_loss = 6.58e-04, train_acc = 99.85%, time spent during this epoch = 10.95s, total time spent = 153.34s\n",
      "epoch 15/15, train_loss = 3.47e-04, train_acc = 99.85%, time spent during this epoch = 10.93s, total time spent = 164.28s\n"
     ]
    }
   ],
   "source": [
    "main_train_conv(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=0.022887831450837906))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.67%\n"
     ]
    }
   ],
   "source": [
    "print((f\"{100*test_our_model_conv():.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are 0.34% better than last time, which is still better than nothing, but there is still a lot of overfitting. It would be interesting to implement a [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) for example to avoid this overfitting!\n",
    "\n",
    "A good value for dropout in a hidden layer is between 0.2 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (DL)",
   "language": "python",
   "name": "myenvdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "689ffbb94fe8f58a5045b4f3f0726e738a118a8a590ae859861904a2cad8ac3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
