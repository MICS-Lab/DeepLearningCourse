{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to redo the RNN to guess what nationalities names come from, with the full original dataset \"names\" instead of \"names_1000\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our alphabet\n",
    "LETTERS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
    "N_LETTERS = len(LETTERS)\n",
    "\n",
    "# Turn a Unicode string to string of characters in our alphabet\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unidecode(s) if c in LETTERS)\n",
    "\n",
    "# Turn a name into a <name_length x 1 x N_LETTERS>, or a tensor of one-hot letter vectors\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, N_LETTERS)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][LETTERS.find(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (unicodeToAscii('Ślusàrski') == 'Slusarski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameToTensor('abcZZZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, filenames: str):\n",
    "        # read data\n",
    "        self.names = []  # X\n",
    "        self.countries = []  # y (strings)\n",
    "        self.country_to_idx = {}  # key: country, value: index\n",
    "        self.idx_to_country = []  # index: index, value: country\n",
    "\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "\n",
    "    def countryID(self, index):\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "\n",
    "# Create object of our custom dataset\n",
    "dataset = NamesDataset('data/names/*.txt')\n",
    "\n",
    "# Split data into train and test with random_split\n",
    "TRAIN_FRACTION = 0.8\n",
    "TRAIN_SIZE = int(TRAIN_FRACTION*len(dataset))\n",
    "TESET_SIZE = len(dataset)-int(TRAIN_FRACTION*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [TRAIN_SIZE, TESET_SIZE])\n",
    "\n",
    "# Store number of countries in a variable\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = \n",
    "test_loader = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network & train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return \n",
    "\n",
    "    def initHidden(self):\n",
    "        return \n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "lr = 0.001\n",
    "optimizer =\n",
    "criterion =\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for (name, country, name_tensor, country_tensor) in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a couple of examples\n",
    "print(f'NAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    confusion[country_tensor.item(), guess_i.item()] += 1\n",
    "    if country_tensor.item() == guess_i.item():\n",
    "        accuracy += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_dataset):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy increased! (~74% vs ~60% last time **and we used an independent test set so it's even more unexpected**)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a couple of examples of our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Dubois\", \"Lhotte\", \"Dupont\", \"Garcia\", \"Sato\", \"Duprès\", \"Suzuki\", \"Wang\", \"Santos\", \"Yamamoto\"]\n",
    "for name in names:\n",
    "    name_tensor = nameToTensor(name)\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {rnn.outputToCountry(output)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost always english/russian that is predicted! Why is that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typical of a class unbalance, let's investigate the size of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cout each country in the dataset with a defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "countr_count = defaultdict(int)\n",
    "total = 0\n",
    "for country in dataset.countries:\n",
    "    countr_count[country] += + 1\n",
    "    total += 1\n",
    "print(f'Country; Count; Percentage')\n",
    "for country, count in countr_count.items():\n",
    "    print(f'{country}; {count}; {100*count/total:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the per-class accuracy because accuracy is not a good metric here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_accuracy_list: list[float] = []\n",
    "for i in range(N_COUNTRIES):\n",
    "    fraction_correct = confusion[i, i].item()\n",
    "    print(f'{dataset.idx_to_country[i]}: {100*fraction_correct:.2f}%')\n",
    "    per_class_accuracy_list.append(100*fraction_correct)\n",
    "print(\"\\n\")\n",
    "print(f'Average per class accuracy: {sum(per_class_accuracy_list)/len(per_class_accuracy_list):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the unbalanced learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did not want to bother you with unbalanced datasets yet as it was your first RNN, all 18 nationalities were represented with the same number of names in TD 6b. Now that we used `names.txt` instead of `names_1000.txt`, we have an unbalanced dataset. This is what real life looks like.\n",
    "\n",
    "We ignored this but because the dataset was smaller from some nationalities, you can see that in the last TD's `Vietnamese.txt`, some names appeared several times, which is almost a way of artificially balancing the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st fixing idea: modify the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading names one by one, choose a country at random, then choose a name at random from this category.\n",
    "Do this using an iterable dataset (checkout the doc here: https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "# Create a custom dataset\n",
    "class NamesIterableDataset(IterableDataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        # Read data\n",
    "        self.names = dict()  # country -> list of names\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            self.names[country] = []\n",
    "            for line in lines:\n",
    "                self.names[country].append(unicodeToAscii(line))\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "\n",
    "    def countryID(self, country):\n",
    "        return torch.tensor(self.country_to_idx[country])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(len(self)):\n",
    "            yield self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return 16059  # Arbitrary, size of one epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx is ignored (why? because of the way we want it to work!), but required by the implementation of __getitem__\n",
    "        \"\"\"\n",
    "        return self.__next__()\n",
    "\n",
    "# Create dataset object\n",
    "dataset = NamesIterableDataset('data/names/*.txt')\n",
    "\n",
    "# Get a sample\n",
    "name, country, name_tensor, country_tensor = next(dataset)\n",
    "name, country, name_tensor.shape, country_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Interpret `name_tensor.shape`?\n",
    "\n",
    "A: `[n_letters_in_name, 1, n_letters_available]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test with random_split\n",
    "TRAIN_FRACTION = 0.8\n",
    "TRAIN_SIZE = int(TRAIN_FRACTION*len(dataset))\n",
    "TESET_SIZE = len(dataset)-int(TRAIN_FRACTION*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [TRAIN_SIZE, TESET_SIZE])\n",
    "\n",
    "# Store number of countries in a variable\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # batch_size 1 as names have different lengths !!!\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)  # shuffle = True is not technically necessary\n",
    "\n",
    "# Get a sample from the dataloader\n",
    "name_train, country, name_tensor, country_tensor = next(iter(train_loader))\n",
    "print(name_train, country, name_tensor.shape, country_tensor)\n",
    "name_test, country, name_tensor, country_tensor = next(iter(test_loader))\n",
    "print(name_test, country, name_tensor.shape, country_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100_000):\n",
    "    name_train, country, name_tensor, country_tensor = next(iter(train_loader))\n",
    "    name_test, country, name_tensor, country_tensor = next(iter(test_loader))\n",
    "    assert name_train != name_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work ... what happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class NamesIterableDataset(IterableDataset):\n",
    "    def __init__(self, filenames='names/*.txt', train: bool = True, train_fraction: float = 0.8):\n",
    "        # Read data\n",
    "        self.names = dict()  # country -> list of names\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            self.names[country] = []\n",
    "            for line in lines:\n",
    "                self.names[country].append(unicodeToAscii(line))\n",
    "        \n",
    "        # Split the data into train and test datasets\n",
    "        \n",
    "\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "\n",
    "\n",
    "    def countryID(self, country):\n",
    "        return torch.tensor(self.country_to_idx[country])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(len(self)):\n",
    "            yield self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Choose random country\n",
    "        country = random.choice(self.idx_to_country)\n",
    "        # Choose random name from this country\n",
    "        name = random.choice(self.names[country])\n",
    "        # Convert to tensors\n",
    "        name_tensor = nameToTensor(name)\n",
    "        countryID = self.countryID(country)\n",
    "        return (name, country, name_tensor, countryID)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 16059  # Arbitrary, size of one epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx is ignored (why? because of the way we want it to work!), but required by the implementation of __getitem__\n",
    "        \"\"\"\n",
    "        return self.__next__()\n",
    "\n",
    "# Create dataset object\n",
    "train_dataset = NamesIterableDataset('data/names/*.txt', train=True, train_fraction=0.8)\n",
    "test_dataset = NamesIterableDataset('data/names/*.txt', train=False, train_fraction=0.8)\n",
    "\n",
    "# Get a sample\n",
    "name, country, name_tensor, country_tensor = next(train_dataset)\n",
    "name, country, name_tensor.shape, country_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store number of countries in a variable\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)  # shuffle is not a thing\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)  # shuffle is not a thing\n",
    "\n",
    "# Get a sample from the dataloader\n",
    "name_train, country, name_tensor, country_tensor = next(iter(train_loader))\n",
    "print(name_train, country, name_tensor.shape, country_tensor)\n",
    "name_test, country, name_tensor, country_tensor = next(iter(test_loader))\n",
    "print(name_test, country, name_tensor.shape, country_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100_000):\n",
    "    name_train, country, name_tensor, country_tensor = next(iter(train_loader))\n",
    "    name_test, country, name_tensor, country_tensor = next(iter(test_loader))\n",
    "    assert name_train != name_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whaaaat? It still doesn't work. Hint: Fakhoury."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw, we see that when we said above we were unbiased and corrected what was done last time, we were not completely honest (and therefore the accuracies were not representative of what would happen on an independent dataset). Never believe everything you read!!! ... Although for this particular task, overfitting a bit is not that big of a deal as there is a finite set of names... but for comparison purposes, it's just a bit stupid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class NamesIterableDataset(IterableDataset):\n",
    "    def __init__(self, filenames='names/*.txt', train: bool = True, train_fraction: float = 0.8):\n",
    "        # Read data\n",
    "        self.names = dict()  # country -> list of names\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            self.names[country] = []\n",
    "            for line in lines:\n",
    "                self.names[country].append(unicodeToAscii(line))\n",
    "        \n",
    "        # Split the data into train and test datasets\n",
    "        for country in self.names.keys():\n",
    "            ###\n",
    "            split_idx = int(train_fraction * len(self.names[country]))\n",
    "            if train:\n",
    "                self.names[country] = self.names[country][:split_idx]\n",
    "            else:\n",
    "                self.names[country] = self.names[country][split_idx:]\n",
    "\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "\n",
    "\n",
    "    def countryID(self, country):\n",
    "        return torch.tensor(self.country_to_idx[country])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(len(self)):\n",
    "            yield self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Choose random country\n",
    "        country = random.choice(self.idx_to_country)\n",
    "        # Choose random name from this country\n",
    "        name = random.choice(self.names[country])\n",
    "        # Convert to tensors\n",
    "        name_tensor = nameToTensor(name)\n",
    "        countryID = self.countryID(country)\n",
    "        return (name, country, name_tensor, countryID)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 16059  # Arbitrary, size of one epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx is ignored (why? because of the way we want it to work!), but required by the implementation of __getitem__\n",
    "        \"\"\"\n",
    "        return self.__next__()\n",
    "\n",
    "# Create dataset object\n",
    "train_dataset = NamesIterableDataset('data/names/*.txt', train=True, train_fraction=0.8)\n",
    "test_dataset = NamesIterableDataset('data/names/*.txt', train=False, train_fraction=0.8)\n",
    "\n",
    "# Get a sample\n",
    "name, country, name_tensor, country_tensor = next(train_dataset)\n",
    "name, country, name_tensor.shape, country_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store number of countries in a variable\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)  # shuffle is not a thing\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)  # shuffle is not a thing\n",
    "\n",
    "# Get a sample from the dataloader\n",
    "name_train, country, name_tensor, country_tensor = next(iter(train_loader))\n",
    "print(name_train, country, name_tensor.shape, country_tensor)\n",
    "name_test, country, name_tensor, country_tensor = next(iter(test_loader))\n",
    "print(name_test, country, name_tensor.shape, country_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100_000):\n",
    "    name_train, country, name_tensor, country_tensor = next(iter(train_loader))\n",
    "    name_test, country, name_tensor, country_tensor = next(iter(test_loader))\n",
    "    assert (name_train != name_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whaaat? Hint: Murphy. It is what it is though, it's not a us problem this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100_000):\n",
    "    name_train, country_train, name_tensor, country_tensor = next(iter(train_loader))\n",
    "    name_test, country_test, name_tensor, country_tensor = next(iter(test_loader))\n",
    "    assert ((name_train, country_train) != (name_test, country_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least we're sure we didn't make anything stupid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-define the RNN and re-train it with our new fancy iterable dataset.\n",
    "Print the final accuracy, and plot the confusion matrix (which should be closer to the identity matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden = self.relu(hidden)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# Train the network\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for (name, country, name_tensor, country_tensor) in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# Test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_loader):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the accuracy went down again, but at least, our RNN isn't biased (hopefully ... let's make sure it actually isn't ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Dubois\", \"Lhotte\", \"Dupont\", \"Garcia\", \"Sato\", \"Duprès\", \"Suzuki\", \"Wang\", \"Santos\", \"Yamamoto\"]\n",
    "for name in names:\n",
    "    name_tensor = nameToTensor(name)\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {rnn.outputToCountry(output)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the per-class accuracy as explained previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_accuracy_list: list[float] = []\n",
    "for i in range(N_COUNTRIES):\n",
    "    n_correct = confusion[i, i].item()\n",
    "    n_total = confusion[i].sum().item()\n",
    "    print(f'{dataset.idx_to_country[i]}: {100*n_correct/n_total:.2f}%')\n",
    "    per_class_accuracy_list.append(100*n_correct/n_total)\n",
    "print(\"\\n\")\n",
    "print(f'Average per class accuracy: {sum(per_class_accuracy_list)/len(per_class_accuracy_list):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.names[\"French\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.names[\"French\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class NamesIterableDataset(IterableDataset):\n",
    "    def __init__(self, filenames='names/*.txt', train: bool = True, train_fraction: float = 0.8):\n",
    "        # Read data\n",
    "        self.names = dict()  # country -> list of names\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            self.names[country] = []\n",
    "            for line in lines:\n",
    "                self.names[country].append(unicodeToAscii(line))\n",
    "        \n",
    "        # Split the data into train and test datasets\n",
    "        for country in self.names.keys():\n",
    "            self.names[country] = (list(set(self.names[country])))  # Remove duplicates\n",
    "            #####\n",
    "            split_idx = int(train_fraction * len(self.names[country]))\n",
    "            if train:\n",
    "                self.names[country] = self.names[country][:split_idx]\n",
    "            else:\n",
    "                self.names[country] = self.names[country][split_idx:]\n",
    "\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "\n",
    "\n",
    "    def countryID(self, country):\n",
    "        return torch.tensor(self.country_to_idx[country])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(len(self)):\n",
    "            yield self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Choose random country\n",
    "        country = random.choice(self.idx_to_country)\n",
    "        # Choose random name from this country\n",
    "        name = random.choice(self.names[country])\n",
    "        # Convert to tensors\n",
    "        name_tensor = nameToTensor(name)\n",
    "        countryID = self.countryID(country)\n",
    "        return (name, country, name_tensor, countryID)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 16059  # Arbitrary, size of one epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx is ignored (why? because of the way we want it to work!), but required by the implementation of __getitem__\n",
    "        \"\"\"\n",
    "        return self.__next__()\n",
    "\n",
    "# Create dataset object\n",
    "train_dataset = NamesIterableDataset('data/names/*.txt', train=True, train_fraction=0.8)\n",
    "test_dataset = NamesIterableDataset('data/names/*.txt', train=False, train_fraction=0.8)\n",
    "\n",
    "# Get a sample\n",
    "name, country, name_tensor, country_tensor = next(train_dataset)\n",
    "name, country, name_tensor.shape, country_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    name, country, name_tensor, country_tensor = next(train_dataset)\n",
    "    if country == \"French\":\n",
    "        print(name, country, name_tensor.shape, country_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(200):\n",
    "    name, country, name_tensor, country_tensor = next(test_dataset)\n",
    "    if country == \"French\":\n",
    "        print(name, country, name_tensor.shape, country_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100_000):\n",
    "    name_train, country_train, name_tensor, country_tensor = next(iter(train_loader))\n",
    "    name_test, country_test, name_tensor, country_tensor = next(iter(test_loader))\n",
    "    assert ((name_train, country_train) != (name_test, country_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden = self.relu(hidden)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# Train the network\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for (name, country, name_tensor, country_tensor) in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# Test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_loader):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Dubois\", \"Lhotte\", \"Dupont\", \"Garcia\", \"Sato\", \"Duprès\", \"Suzuki\", \"Wang\", \"Santos\", \"Yamamoto\"]\n",
    "for name in names:\n",
    "    name_tensor = nameToTensor(name)\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {rnn.outputToCountry(output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class_accuracy_list: list[float] = []\n",
    "for i in range(N_COUNTRIES):\n",
    "    n_correct = confusion[i, i].item()\n",
    "    n_total = confusion[i].sum().item()\n",
    "    print(f'{dataset.idx_to_country[i]}: {100*n_correct/n_total:.2f}%')\n",
    "    per_class_accuracy_list.append(100*n_correct/n_total)\n",
    "print(\"\\n\")\n",
    "print(f'Average per class accuracy: {sum(per_class_accuracy_list)/len(per_class_accuracy_list):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second (more or less equivalent) \"fix\": add a sampler to the dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A third (more or less equivalent) \"fix\": weight the loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_NLP_2022-2023",
   "language": "python",
   "name": "dl_nlp_2022-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a61f982c8ae83496d3304782998ac96a47f5fcf9bfc174247e19a8162c5490e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
