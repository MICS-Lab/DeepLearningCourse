{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to redo the RNN guessing the names' nationalities, with the full original dataset \"names\" instead of \"names_1000\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import unicodedata\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create alphabet, and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network & train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network and show `Name;Truth,Predicted` on several examples.\n",
    "\n",
    "Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a couple of examples\n",
    "print(f'NAME; TRUTH; PREDICTED')\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "# Plot confusion matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy increased! (~75% vs ~60% last time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a couple of examples of our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Dupont\", \"Garcia\", \"Sato\", \"Dupr√®s\", \"Suzuki\", \"Wang\", \"Santos\", \"Yamamoto\"]\n",
    "for name in names:\n",
    "    name_tensor = nameToTensor(name)\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {rnn.outputToCountry(output)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost always english/russian that is predicted!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typical of a class unbalance, let's investigate the size of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cout each country in the dataset\n",
    "countr_count = {}\n",
    "total = 0\n",
    "for country in dataset.countries:\n",
    "    countr_count[country] = countr_count.get(country, 0) + 1\n",
    "    total += 1\n",
    "countr_count, total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the unbalanced learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did not want to bother you with unbalanced datasets yet as it was your first RNN, all 18 nationalities were represented with the same number of names in TD 6b. Now that we used `names.txt` instead of `names_1000.txt`, we have an unbalanced dataset. This is what real life looks like.\n",
    "\n",
    "We ignored this but because the dataset was smaller from some nationalities, you can see that in the last TD's `Vietnamese.txt`, some names appeared several times, which is almost a way of artificially balancing the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st fixing idea: modify the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading names one by one, choose a country at random, then choose a name at random from this category.\n",
    "Do this using an iterable dataset (checkout the doc here: https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "# Create a custom dataset\n",
    "class NamesIterableDataset(IterableDataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        # Read data\n",
    "        self.names = dict()\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            self.names[country] = []\n",
    "            for line in lines:\n",
    "                self.names[country].append(unicodeToAscii(line))\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, country):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[country]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, country):\n",
    "        return torch.tensor(self.country_to_idx[country])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Choose random country\n",
    "        country = random.choice(self.idx_to_country)\n",
    "        # Choose random name from this country\n",
    "        name = random.choice(self.names[country])\n",
    "        # Convert to tensors\n",
    "        name_tensor = nameToTensor(name)\n",
    "        countryID = self.countryID(country)\n",
    "        return (name, country, name_tensor, countryID)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 25000  # Arbitrary, size of one epoch\n",
    "\n",
    "dataset = NamesIterableDataset('data/names/*.txt')\n",
    "# Get a sample\n",
    "name, country, name_tensor, country_tensor = next(dataset)\n",
    "name, country, name_tensor.shape, country_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-define the RNN and re-train it with our new fancy iterable dataset.\n",
    "Print the final accuracy, and plot the confusion matrix (which should be closer to the identity matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = DataLoader(dataset, batch_size=1)  # batch_size 1 as names have different lengths\n",
    "test_loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# Train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for (name, country, name_tensor, country_tensor) in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# Test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_loader):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the accuracy went down again, but at least, our RNN isn't biased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training and testing set are the same, so the testing is still not solid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd fixing idea: add a sampler to the dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse our previous dataset, and a sampler to the dataloader.\n",
    "Checkout the doc [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        #  Read data\n",
    "        self.names = []\n",
    "        self.countries = []\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for line in lines:\n",
    "                self.names.append(unicodeToAscii(line))\n",
    "                self.countries.append(country)\n",
    "        self.n = len(self.names)\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, index):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[self.countries[index]]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, index):\n",
    "        return torch.tensor(self.country_to_idx[self.countries[index]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.names[index], self.countries[index], \\\n",
    "            nameToTensor(self.names[index]), self.countryID(index))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "dataset = NamesDataset('data/names/*.txt')\n",
    "# Split data into train and test with random_split\n",
    "train_fraction = 0.8\n",
    "train_size = int(train_fraction*len(dataset))\n",
    "test_size = len(dataset)-int(train_fraction*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "# Create a random sampler that counterbalances the classes\n",
    "country_weights = torch.tensor([1/train_dataset.dataset.countries.count(country) for country in train_dataset.dataset.idx_to_country])\n",
    "# Train\n",
    "train_items_weights = torch.tensor(\n",
    "    [country_weights[train_dataset.dataset.country_to_idx[country]]\n",
    "    for i,country in enumerate(train_dataset.dataset.countries) if i in train_dataset.indices]\n",
    ")\n",
    "train_sampler = WeightedRandomSampler(train_items_weights, len(train_dataset), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=1)\n",
    "# Test\n",
    "test_items_weights = torch.tensor([country_weights[test_dataset.dataset.country_to_idx[country]] \\\n",
    "    for i,country in enumerate(test_dataset.dataset.countries) if i in test_dataset.indices])\n",
    "test_sampler = WeightedRandomSampler(test_items_weights, len(test_dataset), replacement=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# Train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for name, country, name_tensor, country_tensor in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# Test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_dataset):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on training set\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in train_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(train_dataset):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix ON TRAINING SET')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is higher, but the confusion matrix is more polarized; this technique only partially compensate for the class inbalance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd fixing idea: weight the loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse our previous dataset, dataloader, but modify the loss function to compensate the class sizes.\n",
    "Checkout the doc here: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, filenames='names/*.txt'):\n",
    "        #read data\n",
    "        self.names = []\n",
    "        self.countries = []\n",
    "        self.country_to_idx = {}\n",
    "        self.idx_to_country = []\n",
    "        for filename in glob.glob(filenames):\n",
    "            country = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.country_to_idx[country] = len(self.country_to_idx)\n",
    "            self.idx_to_country.append(country)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for line in lines:\n",
    "                self.names.append(unicodeToAscii(line))\n",
    "                self.countries.append(country)\n",
    "        self.n = len(self.names)\n",
    "        self.n_countries = len(self.country_to_idx)\n",
    "    \n",
    "    def countryTensor(self, index):\n",
    "        tensor = torch.zeros(1, self.n_countries)\n",
    "        tensor[0][self.country_to_idx[self.countries[index]]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def countryID(self, index):\n",
    "        return torch.tensor(self.country_to_idx[self.countries[index]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.names[index], self.countries[index], \\\n",
    "            nameToTensor(self.names[index]), self.countryID(index))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "dataset = NamesDataset('data/names/*.txt')\n",
    "# Split data into train and test with random_split\n",
    "train_fraction = 0.8\n",
    "train_size = int(train_fraction*len(dataset))\n",
    "test_size = len(dataset)-int(train_fraction*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "N_COUNTRIES = dataset.n_countries\n",
    "\n",
    "# Create a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # batch_size 1 as names have different lengths\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Create the network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, idx_to_country):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.idx_to_country = idx_to_country\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    def outputToCountry(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        return self.idx_to_country[top_i[0,0].item()]\n",
    "\n",
    "    def outputToID(self, output):\n",
    "        _, top_i = output.topk(1)\n",
    "        return top_i[0,0].item()\n",
    "\n",
    "N_HIDDEN = 128\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_COUNTRIES, dataset.idx_to_country)\n",
    "\n",
    "# Train the network\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "country_weights = torch.tensor([1/train_dataset.dataset.countries.count(country) for country in train_dataset.dataset.idx_to_country])\n",
    "criterion = nn.NLLLoss(country_weights)\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    for name, country, name_tensor, country_tensor in train_loader:\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "        for i in range(name_tensor.size()[1]):\n",
    "            output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "        loss = criterion(output, country_tensor[0][None])\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} ({100*(epoch+1)/n_epochs:.0f}%)\\tLoss: {loss_sum:.6f}')\n",
    "\n",
    "# Test on a couple of examples\n",
    "print(f'\\nNAME; TRUTH; PREDICTED')\n",
    "for i in range(10):\n",
    "    name, country, name_tensor, country_tensor = test_dataset[i]\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[i], hidden)\n",
    "    print(f'{name}; {country}; {rnn.outputToCountry(output)}')\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in test_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(test_dataset):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on training set\n",
    "confusion = torch.zeros(N_COUNTRIES, N_COUNTRIES)\n",
    "accuracy = 0\n",
    "for name, country, name_tensor, country_tensor in train_loader:\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    for i in range(name_tensor.size()[1]):\n",
    "        output, hidden = rnn(name_tensor[0][i], hidden)\n",
    "    guess, guess_i = output.topk(1)\n",
    "    category_i = country_tensor[0][None]\n",
    "    confusion[category_i, guess_i] += 1\n",
    "    if category_i == guess_i:\n",
    "        accuracy += 1\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(N_COUNTRIES):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "# Accuracy\n",
    "print(f'\\nAccuracy: {100*accuracy/len(train_dataset):.2f}%')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(confusion.numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Confusion matrix ON TRAINING SET')\n",
    "ax = plt.gca()\n",
    "positions = list(range(N_COUNTRIES))\n",
    "labels = train_dataset.dataset.idx_to_country\n",
    "small_labels = [label[:2] for label in labels]\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(small_labels))\n",
    "ax.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
    "ax.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_NLP_2022-2023",
   "language": "python",
   "name": "dl_nlp_2022-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a61f982c8ae83496d3304782998ac96a47f5fcf9bfc174247e19a8162c5490e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
