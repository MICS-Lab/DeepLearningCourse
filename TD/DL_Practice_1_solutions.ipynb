{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovDJUER3rFK4"
      },
      "source": [
        "# Manual Implementation of Deep Learning - TD 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmJfTThvBnc"
      },
      "source": [
        "[Use numpy for all questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "id": "R9k-Z90ivZfW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBCErfllsUwJ"
      },
      "source": [
        "## Linear Regression\n",
        "**We will do a linear regression using backpropagartion.**\n",
        "*Of course, there is an analyctical solution to this problem, but the techniques used can be generalized to complex models, for which we do not have an analyctical solution to compute the optimal weights.* We forget about activation functions for now because we do not need non-linearity (we are trying to fit a linear function!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3kDEQAQrM0n"
      },
      "source": [
        "Create $N \\in \\mathbb{N}$ points of data $x_i, 0 \\leq i < N$ with $x_i \\in [0,1)$ and associated (noised) output $y_i = a*x_i+b+\\epsilon_i$ with $\\epsilon_i \\approx \\mathcal{N}(0,0.01)$. Use the function `np.random.randn`.\n",
        "\n",
        "Use $a=2$, $b=1$ & $N=100$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "fJl43jGfqttg"
      },
      "outputs": [],
      "source": [
        "# Data Generation\n",
        "np.random.seed(42)\n",
        "xs = np.random.rand(100, 1)\n",
        "ys = 2 * xs + 1 + .1 * np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLRmzkIs0Xuz"
      },
      "source": [
        "Plot the dots to check it makes sense to use a linear model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "VH2DcGt9vRKB",
        "outputId": "e59175e1-8991-4cc0-b150-8ed9c3e984f9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(xs, ys, '.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knMLtSZJvTs4"
      },
      "source": [
        "Initializes parameters $a$ and $b$ randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM7M1w1Hvqoi",
        "outputId": "92afc24b-7510-4679-9258-0fd2b1966b0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.49671415]), array([-0.1382643]))"
            ]
          },
          "execution_count": 339,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "a = np.random.randn(1)\n",
        "b = np.random.randn(1)\n",
        "a,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu3fnwJkwTFZ"
      },
      "source": [
        "Calculate your predictions $\\hat{y} = a * x_{train} +b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "id": "HOV76bZvvzIF"
      },
      "outputs": [],
      "source": [
        "yhat = a * xs + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IioTqYCYvqFK"
      },
      "source": [
        "Calculate the error $err = \\hat{y} - y_{train}$ and the MSE loss defined as: $loss = \\frac{err^2}{N}$ (dividing by $N$ is not actually necessary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "id": "t_U-eMawz3I8"
      },
      "outputs": [],
      "source": [
        "error = (yhat - ys)\n",
        "loss = (error ** 2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY7QH55Az2zN"
      },
      "source": [
        "Compute the gradients for both $a$ and $b$ parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "id": "JVcVSdWw1C62"
      },
      "outputs": [],
      "source": [
        "a_grad = 2 * (xs * error).mean()\n",
        "b_grad = 2 * error.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqLpi_Iz1Hbf"
      },
      "source": [
        "Update parameters using gradients and the learning rate `lr = 1e-1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRIFn6o_1HQv",
        "outputId": "652fc665-aac8-44a3-81b9-50ba39d3cfb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.69574465]), array([0.23073016]))"
            ]
          },
          "execution_count": 343,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = 1e-1\n",
        "a = a - lr * a_grad\n",
        "b = b - lr * b_grad\n",
        "a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V5p13y11mab"
      },
      "source": [
        "Repeat this for $1000$ epochs and plot error as a function of epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOpyKjXt1q-b",
        "outputId": "8e12d497-c616-4869-bc02-4154c7eb67ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1.95402224]), array([1.02150984]))"
            ]
          },
          "execution_count": 344,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # compute our model's predicted output\n",
        "    yhat = a * xs + b\n",
        "    \n",
        "    # compute the error\n",
        "    error = (yhat - ys)\n",
        "    # compute the loss (MSE)\n",
        "    loss = (error ** 2).mean()\n",
        "    \n",
        "    # compute gradients for both a and b parameters\n",
        "    a_grad = 2 * (xs * error).mean()\n",
        "    b_grad = 2 * error.mean()\n",
        "    \n",
        "    # update parameters using gradients and the learning rate\n",
        "    a = a - lr * a_grad\n",
        "    b = b - lr * b_grad\n",
        "\n",
        "a,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUchKsJN13AB"
      },
      "source": [
        "Check that we get the same results as our gradient descent with an analytical linear regression (you may use `sklearn.linear_model.LinearRegression`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUcdhnb92JHH",
        "outputId": "39d0da1b-af9b-486f-861d-754a2d543aaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1.95402268]), array([1.02150962]))"
            ]
          },
          "execution_count": 345,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "linr = LinearRegression()\n",
        "linr.fit(xs, ys)\n",
        "linr.coef_[0], linr.intercept_  # a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojpv-bEx27tB"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCSvgvL85fcb"
      },
      "source": [
        "## $XOR$\n",
        "**We will create a (simple) neural network with 2 layers to fit the $XOR$ function.**\n",
        "*Of course, this is dumb since the xor function is (much) faster than running a neural network; but the technique can be used to fit complex functions, for which we may not even have an expression.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z0SXBVs28y3"
      },
      "source": [
        "Define the $XOR$ function and create some synthetic training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgLvfEg35xMf",
        "outputId": "4ff8ab69-eb6a-48ae-e1f8-517d8e2b0d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ],
      "source": [
        "def xor(x1, x2):\n",
        "    return (x1 and not x2) or (not x1 and x2)\n",
        "\n",
        "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_train = np.array([[xor(x[0], x[1])] for x in X_train])\n",
        "\n",
        "print(X_train)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klWDhYdY5wzq"
      },
      "source": [
        "Define the model's architecture and some hyperparameters:\n",
        "- $10$ hidden units\n",
        "- learning rate of $0.5$\n",
        "- 10_000 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "id": "XkpOr8fM6KpJ"
      },
      "outputs": [],
      "source": [
        "# number of hidden units\n",
        "n_hidden = 10\n",
        "# learning rate\n",
        "lr = 0.2\n",
        "# number of epochs\n",
        "epochs = 10_000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p17F-_wD6UTC"
      },
      "source": [
        "Define the model's weight matrices and biases.\n",
        "Depending on how you perform the calculation between one layer and the other of the neural network ($l_{n} = Wl_{n-1} + b_n$ or $l_{n} = l_{n-1}W + b_n$), the number of rows and columns of W might be inverted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23X5LaDp6Uzh",
        "outputId": "36bfdacd-18ca-404c-8196-355ab08acf0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_hidden\n",
            "[[ 0.49671415 -0.1382643 ]\n",
            " [ 0.64768854  1.52302986]\n",
            " [-0.23415337 -0.23413696]\n",
            " [ 1.57921282  0.76743473]\n",
            " [-0.46947439  0.54256004]\n",
            " [-0.46341769 -0.46572975]\n",
            " [ 0.24196227 -1.91328024]\n",
            " [-1.72491783 -0.56228753]\n",
            " [-1.01283112  0.31424733]\n",
            " [-0.90802408 -1.4123037 ]]\n",
            "b_hidden\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "W_out\n",
            "[[ 1.46564877 -0.2257763   0.0675282  -1.42474819 -0.54438272  0.11092259\n",
            "  -1.15099358  0.37569802 -0.60063869 -0.29169375]]\n",
            "b_out\n",
            "[0.]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# weight and bias matrices for the hidden layer\n",
        "W_hidden = np.random.randn(n_hidden, 2)  # n_hidden rows and two columns, we \n",
        "# are therefore doing W*x with x a column vector (to match the slides).\n",
        "# The same would obviously work with x*W with x a line vector.\n",
        "b_hidden = np.zeros((n_hidden))  # b_hidden can be considered as a vector, \n",
        "# a matrix would work but isn't necessary. It will have to be transposed\n",
        "# as vectors are line vectors in Python.\n",
        "\n",
        "print(\"W_hidden\")\n",
        "print(W_hidden)\n",
        "print(\"b_hidden\")\n",
        "print(b_hidden)\n",
        "\n",
        "# separator\n",
        "print(\"\\n\")\n",
        "\n",
        "# weight and bias matrices for the output layer\n",
        "W_out = np.random.randn(1, n_hidden)\n",
        "b_out = np.zeros((1))\n",
        "\n",
        "print(\"W_out\")\n",
        "print(W_out)\n",
        "print(\"b_out\")\n",
        "print(b_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayNLprId6hkp"
      },
      "source": [
        "Define the sigmoid ($\\sigma (x) = \\frac{1}{1+e^{-x}}$) activation function and its derivative: $\\sigma (x) * (1-\\sigma (x))$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "lQI-ZwOJ6jCE"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZsAoi0c6nUT"
      },
      "source": [
        "Define the forward pass of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "n2KyCoYZ6mJV"
      },
      "outputs": [],
      "source": [
        "def forward(X):\n",
        "    # number of samples\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # hidden layer activations\n",
        "    hidden_activations = np.zeros((n_samples, W_hidden.shape[0]))\n",
        "    # output layer activations\n",
        "    output_activations = np.zeros((n_samples, W_out.shape[0]))\n",
        "    # loss\n",
        "    loss = 0\n",
        "    for i in range(n_samples):\n",
        "        hidden_activations[i] = sigmoid(np.dot(W_hidden, X[i].T) + b_hidden.T)  # layer 1\n",
        "        output_activations[i] = sigmoid(np.dot(W_out, hidden_activations[i]) + b_out)  # y_hat\n",
        "        loss += (y_train[i] - output_activations[i]) ** 2\n",
        "    return hidden_activations, output_activations, loss\n",
        "\n",
        "    # It's also important to note that this solution is not the most efficient, \n",
        "    # especially when you have a large dataset. But for the sake of clarity,\n",
        "    # it is more understandable if we iterate over the samples in X to calculate\n",
        "    # the hidden_activations and output_activations for each sample."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the backward pass of the mode.\n",
        "\n",
        "To determine how parameters will be updated, we need to compute the partial derivatives of the error with regards to each parameter."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRuWEq06upn"
      },
      "source": [
        "\n",
        "\n",
        "Let's compute the derivate with regard to each element of $W_{out}$. $W_{out}$ is a matrix with only one row (because the output is 1D). Which is why we need to compute $\\frac{\\partial e}{\\partial w_{out,1,j}}$ and not $\\frac{\\partial e}{\\partial w_{out,i,j}}$. Let's define the loss we are trying to minimise as $(\\widehat{y} - y_{train})^2$.\n",
        "\n",
        "$\\frac{\\partial e}{\\partial w_{out, 1, j}}(x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= \\frac{\\partial e}{\\partial \\widehat{y}} (\\widehat{y} (x, W_1, b_1, W_{out}, b_{out}))) * \\frac{\\partial \\widehat{y}}{\\partial w_{out, 1, j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\frac{\\partial \\sigma(W_{out}l^1 +b_{out})}{\\partial w_{out, 1, j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * \\frac{\\partial W_{out}l^1 +b_{out}}{\\partial w_{out, 1, j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * \\frac{\\partial W_{out}l^1}{\\partial w_{out, 1, j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * l_{j}$\n",
        "\n",
        "Let's compute the derivative with regard to each element of $b_{out}$ (only one, $j = 1$).\n",
        "\n",
        "$\\frac{\\partial e}{\\partial b_{out, j}}(x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= \\frac{\\partial e}{\\partial \\widehat{y}} (\\widehat{y} (x, W_1, b_1, W_{out}, b_{out}))) * \\frac{\\partial \\widehat{y}}{\\partial b_{out,j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * \\frac{\\partial W_{out}l^1 + b_{out}}{\\partial b_{out, j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out})$\n",
        "\n",
        "Let's compute the derivative with regard to each element of $b_{hidden}$ ($n_{hidden}$ elements).\n",
        "\n",
        "$\\frac{\\partial e}{\\partial b_{hidden, j}}(x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= \\frac{\\partial e}{\\partial \\widehat{y}} (\\widehat{y} (x, W_1, b_1, W_{out}, b_{out}))) * \\frac{\\partial \\widehat{y}}{\\partial b_{hidden,j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * \\frac{\\partial W_{out}l^1 + b_{out}}{\\partial b_{hidden, j}} (x, W_1, b_1, W_{out}, b_{out})$\n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * \\frac{\\partial W_{out} (W_{hidden}x + b_{hidden}) + b_{out}}{\\partial b_{hidden, j}} (x, W_1, b_1, W_{out}, b_{out})$ \n",
        "\n",
        "$= 2*((\\widehat{y}-y_{train})) * \\sigma' (W_{out}l^1 + b_{out}) * W_{out, j}$ \n",
        "\n",
        "\n",
        "We will not show the culcations to get the derivates with regards to $W_{hidden}$ but it really is the same (using the chain rule and being very careful).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "w206oLTf6vP8"
      },
      "outputs": [],
      "source": [
        "def backward(X, y_train, hidden_activations, output_activations):\n",
        "    # global variables because we are going to change them and we need the\n",
        "    # change to only be done locally\n",
        "    global W_hidden, b_hidden, W_out, b_out\n",
        "\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # update weight matrices and biases\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_hidden):\n",
        "            W_out[0][j] -= lr * 2 * (output_activations[i] - y_train[i]) * sigmoid_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * hidden_activations[i][j]\n",
        "        b_out -= lr * 2 * (output_activations[i] - y_train[i]) * sigmoid_derivative(np.dot(W_out, hidden_activations[i]) + b_out)\n",
        "\n",
        "        for j in range(n_hidden):\n",
        "            for k in range(2):  # because W_hidden is a matrix n_hidden rows, 2 columns\n",
        "                W_hidden[j][k] -= lr * 2 * (output_activations[i] - y_train[i]) * sigmoid_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j] * sigmoid_derivative(np.dot(W_hidden, X[i].T) + b_hidden.T)[j] * X[i][k]\n",
        "            b_hidden[j] -= lr * 2 * (output_activations[i] - y_train[i]) * sigmoid_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQziMSRQ67NP"
      },
      "source": [
        "Performs the forward pass, backward pass, and weight updates for $10,000$ epochs;\n",
        "in addition:\n",
        "- print the loss every $10,000$ epochs\n",
        "- save the losses and plot them in a semilogy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "zW_rM4yZ689f",
        "outputId": "c852e197-7e30-480d-e94f-d23f69518db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: loss = [1.3328554]\n",
            "Epoch 1000: loss = [0.05634485]\n",
            "Epoch 2000: loss = [0.0315016]\n",
            "Epoch 3000: loss = [0.02094356]\n",
            "Epoch 4000: loss = [0.01381343]\n",
            "Epoch 5000: loss = [0.00971074]\n",
            "Epoch 6000: loss = [0.00728384]\n",
            "Epoch 7000: loss = [0.00574407]\n",
            "Epoch 8000: loss = [0.00470195]\n",
            "Epoch 9000: loss = [0.00395866]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRc5Z3m8e9bpc3aZW3Waq3ehHfjDTBhidkhCekEhzUhkDChM0nPnDlk+nRPumfS0zMnyZCcyQRIcNJhjQMkAUOaHUzwviGvsizLsiRbq63VlrW980ddOULBC9Zyb1U9n3PqUPXWcn9X1zz31nvfeq+x1iIiIuHB53YBIiIycRT6IiJhRKEvIhJGFPoiImFEoS8iEkYi3C7gXNLS0mxBQYHbZYiIBI1t27a1WGvTz/a8p0O/oKCArVu3ul2GiEjQMMbUnOt5de+IiIQRhb6ISBhR6IuIhBGFvohIGFHoi4iEEYW+iEgYUeiLiISRkAz9X31Yzdryo26XISLiOSEZ+k9vrOFPuxrcLkNExHNCMvQj/T76BgbdLkNExHNCMvT9PsPAoK4IJiIyUkiGfoTfR59CX0Tkr4Rk6Ef6DP3q3hER+SshGfoRfkP/gI70RURGmrCplY0xccD/A3qB96y1z4zXsiL9Prr6+8fr40VEgtaojvSNMauNMU3GmN0j2q83xlQYYw4aYx5xmr8AvGCtfQC4dTTLPR+dyBUR+WSj7d75NXD98AZjjB/4GXADMAtYZYyZBeQCtc7LBka53HNKjImkurmbmtbu8VyMiEjQGVXoW2vXAcdHNC8GDlprD1lre4HngduAOgLBP+rlns/DV5fg9xu+/PhGDjR2jueiRESCyniEbw5/OaKHQNjnAC8Btxtjfg68crY3G2MeNMZsNcZsbW5uvqgCpmUm8NwDSxm0li/+fD1bDo/cL4mIhKcJG71jre221n7VWvvQuU7iWmufsNYustYuSk8/67V9z2tmViIvPrSctPho7vrlJt7Yo2kZRETGI/Trgbxhj3OdtgmXNzmWFx5azoysRL759Dbeq2hyowwREc8Yj9DfApQaYwqNMVHAHcDLn+YDjDG3GGOeaG9vH3Uxk+OieO6BJUzLTOA7v91JfdupUX+miEiwGu2QzeeADcB0Y0ydMeZ+a20/8DDwOrAPWGOt3fNpPtda+4q19sGkpKTRlHdGbFQEP79rIb39g/zjH3ZjrYZzikh4GtWPs6y1q87S/hrw2mg+e6wVpsXx3Wun8YPX9vHm3kZWlk1xuyQRkQkXktMwnM19lxVQnB7Hj944wKB+vCUiYciToT+WffrDRfp9fPuaUioaO3ldo3lEJAx5MvTHuk9/uJvnZFOQGsvqD6vH/LNFRLzOk6E/nvw+w1eW5LPl8Akq9WtdEQkzYRf6ALcvyCXSb1iztfb8LxYRCSFhGfqp8dGsKE3ntV0NGr4pImHFk6E/Xidyh7uubAr1bafYc7Rj3JYhIuI1ngz98TyRO+SamRn4DLyxt3HcliEi4jWeDP2JkBofzdy8ZD482OJ2KSIiEyZsQx9gaVEqH9W20X1al1YUkfAQ1qG/rCiV/kHL1poTbpciIjIhPBn6E3EiF2BRQQoRPsPGQ63juhwREa/wZOhPxIlcCMy+OSMrgY9q28Z1OSIiXuHJ0J9Ic3KT2VXfrgnYRCQshH3oz81NorOnn8Ot3W6XIiIy7sI+9GfnJANQXje+5w9ERLwg7EN/WmY8MZE+dtUr9EUk9Hky9Cdq9A5AhN9HaUYCBzTjpoiEAU+G/kSN3hlSmhlPRYNCX0RCnydDf6JNy0ygqfM07Sf73C5FRGRcKfSB6ZkJABxo0tG+iIQ2hT6B7h1AXTwiEvIU+kBO8iTiovy6fKKIhDyFPmCMoSQzgcqmLrdLEREZV54M/YkcsjmkOC2O6hb9KldEQpsnQ3+ih2wCFKTFcay9h1O9AxO2TBGRiebJ0HdDYVocgObgEZGQptB3DIW+unhEJJQp9B0KfREJBwp9R1x0BBkJ0Qp9EQlpCv1hCjWCR0RCnEJ/mKJ0hb6IhDaF/jCFaXEc7+7VxGsiErIU+sMUpDonczVsU0RClCdD341f5EKgewegukXTMYhIaPJk6Lvxi1yAvMmx+AxUN+tIX0RCkydD3y3REX5yU2I5pJO5IhKiFPojFKfHUaUjfREJUQr9EYrS46lu6WJw0LpdiojImFPoj1CcHk9P3yBH20+5XYqIyJhT6I8wNILnkLp4RCQEKfRHKE4PXC+3qlnDNkUk9Cj0R0iLjyIhJkKhLyIhSaE/gjGG4vR4de+ISEhS6H+CovQ4HemLSEhS6H+C4vR4GjtO03W63+1SRETGlEL/ExSfGcGjo30RCS2eDH23JlwbMjSCR/36IhJqPBn6bk24NiQ/NRa/z1DZ1OnK8kVExosnQ99t0RF+itLi2H9MoS8ioUWhfxYzsxLZ36DQF5HQotA/ixlZCdS3ndKlE0UkpCj0z2JmViIA+xs6XK5ERGTsKPTPYpYT+vuOKfRFJHQo9M8iIyGalNhI9euLSEhR6J+FMYaZWYk60heRkKLQP4dZzgievoFBt0sRERkTCv1zmJuXzOn+QY3XF5GQodA/h/n5yQDsqD3hciUiImNDoX8OOcmTSIuPZueRNrdLEREZEwr9czDGMD8/mZ21Cn0RCQ0K/fOYl5fMoZZu2k72ul2KiMioKfTP40y/vrp4RCQEKPTPY15eMpF+w8bqVrdLEREZNYX+ecRGRTA/L4UNVQp9EQl+Cv0LsKw4ld317ZpxU0SCnkL/AiwvTmXQwiZ18YhIkJuw0DfGFBljnjTGvDBRyxwr8/KTiYn0sV5dPCIS5C4o9I0xq40xTcaY3SParzfGVBhjDhpjHjnXZ1hrD1lr7x9NsW6JjvCzuDCVdQea3S5FRGRULvRI/9fA9cMbjDF+4GfADcAsYJUxZpYxZrYxZu2IW8aYVu2Ca2dmcKilm4NNXW6XIiJy0S4o9K2164DjI5oXAwedI/he4HngNmvtLmvtzSNuTRdakDHmQWPMVmPM1uZm7xxZXzszE4C39jW6XImIyMUbTZ9+DlA77HGd0/aJjDGpxpjHgPnGmO+d7XXW2iestYustYvS09NHUd7Yyk6eRFl2Im/uVeiLSPCKmKgFWWtbgW9O1PLGw2dnZfKTtytp7jxNekK02+WIiHxqoznSrwfyhj3OddpGzRhzizHmifb29rH4uDFz4+wsrIW15UfdLkVE5KKMJvS3AKXGmEJjTBRwB/DyWBRlrX3FWvtgUlLSWHzcmJmWmUBZdiK/3zEm+zYRkQl3oUM2nwM2ANONMXXGmPuttf3Aw8DrwD5gjbV2z/iV6g2fn59DeV27RvGISFC60NE7q6y1WdbaSGttrrX2Saf9NWvtNGttsbX2B+NbqjfcOi8bn4GXtte5XYqIyKfmyWkYvNqnD5CREMNV0zNYs7WW0/0DbpcjIvKpeDL0vdqnP+Te5QW0dPXy2q5jbpciIvKpeDL0ve7ykjSK0uP49foat0sREflUFPoXwecz3Le8gI9q29hcPfKHyiIi3qXQv0h/szCPtPhoHn3rgNuliIhcME+GvpdP5A6ZFOXnm1cWsb6qlU2HNOWyiAQHT4a+10/kDrlzyVTS4qP50RsHsNa6XY6IyHl5MvSDxaQoP9+5tpTNh4+ztlwjeUTE+xT6o7RqcT5l2Yn8y2v7ONnb73Y5IiLnpNAfJb/P8E+3lnGsvYcfv6GTuiLibZ4M/WA4kTvcooLJ3Lkknyc/rGajTuqKiId5MvSD5UTucP/1xpnkT47lP635iI6ePrfLERH5RJ4M/WAUFx3Bj780l4aOHv7utzsZHNRoHhHxHoX+GFo4dTL/cNNM3trXxKNvV7pdjojIX5mwyyWGi3uXF7D7aAc/fbuS3JRJfGlR3vnfJCIyQRT6Y8wYww8+fwmNHT088mI5CdER3DA7y+2yREQAj3bvBNvonZGiI/w8fvdC5uen8O3nd+iauiLiGZ4M/WAcvTNSbFQEq++7lHl5yfztczt4aqOmYRYR93ky9ENF0qRIfvO1JVw9PYN/+MNu/vmVvfQNDLpdloiEMYX+OJsU5eexuxdy3/ICVn9YzZ2/3ERTZ4/bZYlImFLoT4BIv4/v31rGo1+eR3ldG9f9n3W8qgnaRMQFCv0J9Ln5Oaz928vJnxzLt57dzsPPbtdRv4hMKIX+BCvJSODFh5bzn1dO4/U9DVz9w/d5Yl0Vvf3q6xeR8efJ0A/2IZvnE+H38fDVpbzx3StZXDiZf3ltP9c/uo615Uc1fYOIjCvj5Ss+LVq0yG7dutXtMsbdu/ub+MFr+zjY1MWMKQl859ppXFeWiTHG7dJEJMgYY7ZZaxed9XmFvjcMDFrWlh/l0bcqqW7pZsaUBB5cUcTNc7KJivDkFzIR8SCFfpDpHxjkDzuP8vj7VVQ2dTElMYavXlbAqiX5JMZEul2eiHicQj9IWWt570Azv/zgEB8ebCUuys/tC3O5a+lUpmUmuF2eiHiUQj8E7K5vZ/WH1awtP0Zv/yBLiyZz99ICVpZlEulX14+I/IVCP4Qc7+5lzdZant5YQ92JU2QkRHPH4ny+sjifKUkxbpcnIh6g0A9BA4OW9w808dSGGt470IzPGFbOyuTuZVNZVpSqUT8iYex8oa/59IOQ32e4ekYmV8/I5EjrSZ7ZXMNvt9Typ90NlGbEc8+yqXx+QS7x0dq8IvJxOtIPET19A7zy0VF+s6GGXfXtxEdH8IUFOdyzbColGTrxKxIugrJ7xxhzC3BLSUnJA5WVutbsp2GtZWdtG09tqAmc+B0YZHlxKvcsK+DamRlE6MSvSEgLytAfoiP90WntOs3zW2p5dtMR6ttOkZ0Uw1eW5HPH4nzS4qPdLk9ExoFCX+gfGOSd/U38ZkMNfz7YQqTfcMvcbL5+eRGzshPdLk9ExpBO5AoRfh8ry6awsmwKVc1d/Gb9YX63rY6XttdzeUka919RyGempWvUj0gY0JF+mGo/2cezm4/w6/XVNHacpjQjnq9fUcht83KIifS7XZ6IXCR178g59fYPsrb8KL/4oJp9xzpIi4/i7qUF3LU0n1T1+4sEHYW+XBBrLRuqWvnFB4d4t6KZ6Agfty/M5cEriihIi3O7PBG5QOrTlwtijGF5SRrLS9I42NTJk3+u5oWtdTy/+Qg3zs7im1cWc0lOkttlisgo6Uhfzqqpo4fVHx7m6Y01dJ3u54rSNB76TLGmehDxMHXvyKi1n+rjmU01rP7zYVq6TjM3L5mHrixi5awp+HwKfxEvUejLmOnpG+DF7XU8/v4hjhw/SVF6HN9cUcxt87OJjtCIHxEvUOjLmOsfGORPuxt47P0q9hztIDMxmq9fXsSqJfma5E3EZQp9GTfWWj6obOHn71Wx4VAriTER3LOsgPsuK9A0DyIuCcrQ14RrwWfHkRM89n4Vb+xtJMrv40uL8njgiiLyU2PdLk0krARl6A/RkX7wOdjUxePvV/GHnfUMDFpunJ3FN1YUMztXwz1FJoJCX1zR0N7Dr9ZX8+zGI3Se7ueyklQeXFHMitI0DfcUGUcKfXFVR08fz206wuoPA3P8zMxK5BsrirhpTpYu6i4yDhT64gmn+wf4486j/GLdISqbushJnsT9lxfy5UvziNOIH5Exo9AXTxkctLxb0cTj7x9i8+HjJE2K5J5lU7l3uUb8iIwFhb541raaEzyxLjDiJ9Lv44sLc3ngiiIKNcGbyEVT6IvnHWru4hcfVPPi9jr6Bga5ZkYmX7u8QHP8iFwEhb4EjabOHp7aUMMzm45wvLuXGVMS+Nrlhdw6N1sXdhG5QAp9CTo9fQO8vPMoqz+sZn9DJ6lxUdy5JJ+7lk4lIzHG7fJEPE2hL0Fr6MIuqz88zNv7G4nwGW6Zk81XLyvUj71EzkIXUZGgNfzCLodbuvn1+sP8bmstL+2o59KCFO5dXsDKWVOIitB4f5ELpSN9CSodPX2s2VLLv204TO3xU6TFR7NqcR6rFueTnTzJ7fJEXKfuHQlJA4OWdQeaeWpjDe9WNGGAa2ZmctfSqVxRkqaLu0jYUveOhCS/z3DVjAyumpFB7fGTPLv5CGu21PLm3kampsZy55J8/mZhHilxUW6XKuIpOtKXkHG6f4B/393A0xtr2HL4BFERPm6ek8WqxfksmpqiMf8SFtS9I2Fpf0MHT2+s4ffb6+nuHaAoPY4vLcrj9gW5pCdougcJXQp9CWvdp/t5ddcx1mypZWvNCSJ8hqtnZPDlS/O4clo6EZrpU0KMQl/EcbCpi99treXF7XW0dPWSmRjN7Qty+dKiPAo034+ECIW+yAh9A4O8s7+JNVtqebeiiUELSwon84UFOdwwO4vEmEi3SxS5aJ4JfWPM54CbgETgSWvtG+d7j0JfxltjRw8vbKvjxW11HGrpJirCx2dnZvL5+TlcOT1dF3qRoDMmoW+MWQ3cDDRZay8Z1n498BPAD/zSWvuvF/BZKcAPrbX3n++1Cn2ZKNZaPqpr5/fb63il/BjHu3uZHBfFLXOy+Nz8HOblJWv0jwSFsQr9FUAX8Juh0DfG+IEDwGeBOmALsIrADuB/jviIr1lrm5z3/Qh4xlq7/XzLVeiLG/oGBll3oJmXdtTz5t5GevsHKUqL43Pzc7htXjZTU9X/L941Zt07xpgCYO2w0F8GfN9ae53z+HsA1tqRgT/0fgP8K/CmtfatcyznQeBBgPz8/IU1NTUXVJ/IeOjo6eNPu47x0vZ6NlUfB2BObhI3zc7ipjlZ5KbEulyhyMeNZ+h/EbjeWvt15/HdwBJr7cNnef+3gXsJfCPYaa197HzL1JG+eEl92yleLT/K2vJjlNe1AzA/P5mb52Rz0+wspiRp2mdxn2emYbDW/hT46UQtT2Ss5SRP4sEVxTy4opia1m7Wlh/j1fJj/Pe1e/kfr+7l0qmTuXluFjdckqUfgIlnjSb064G8YY9znTaRkDc1NY5vXVXCt64qoaq5i1edHcA//nEP3395D4sLJ3Nd2RRWlk0hR7N/ioeMpnsngsCJ3GsIhP0W4CvW2j2jLsqYW4BbSkpKHqisrBztx4lMmAONnawtP8a/7z7GgcYuAGbnJHFdWSbXlU2hJCNeo4BkXI3V6J3ngM8AaUAj8N+stU8aY24EHiUwYme1tfYHY1K1Q336EsyqW7p5fU8Dr+9pYMeRNgCK0uJYWTaF68oymZubrCmgZcx55sdZF0OhL6GisaOHN/Y28saeBjZUtdI/aJmSGMNnZ2VyzcwMlhal6uLvMiYU+iIe036yj3cqGnl9dyPvH2jmVN8AkyL9XFaSxjUzM7hqeoZGAslFC8rQV5++hIuevgE2HGrl3f1NvL2vifq2UwCUZSdy9YwMrp6RoW4g+VSCMvSH6Ehfwom1lsqmLt7e18S7+5vYWnOcQQupcVF8ZnpgB3B5SRpJsZoQTs5OoS8SpNpO9vL+gWbe2d/EexXNtJ/qw2dgbl4yK0rTWTEtjbm5ybomgHyMQl8kBPQPDLKzto11lS2sO9BMeV0bgxYSYiK4rDiNK6alsaI0nbzJmhYi3AVl6KtPX+Tc2k72sr6qlXUHmll3oJmj7T0AFKbFcUVpYAewtDiV+OgJ+9G9eERQhv4QHemLnJ+1lqrmbj6obOaDyhY2VLVyqm8Av88wJzeJZUWpLC9OY+HUFCZFaVhoqFPoi4SZ0/0DbDt8gvVVrayvaqG8rp3+QUuU38e8/GRnJ5DKvPxkoiO0Ewg1Cn2RMNd1up8th4+zsaqV9VWt7D7ajrUQE+lj4dQUlhensbQolTm5SbpSWAhQ6IvIx7Sf7GNTdSsbDrWyoaqV/Q2dAMRG+VmQn8KlBZO5tDCF+XnqDgpGQRn6OpErMnFau06zqfo4Gw+1srn6OBWNnVgLkX7DJTlJLC6YzKUFk1lUkEJybJTb5cp5BGXoD9GRvsjEaz/Vx7aa42yuPsHWw8cpr2und2AQgOmZCVxaGPg2sLhwMllJmjbaaxT6IjIqPX0DfFTbxpbDx9l8+ATba07QdbofCFxYZn5+MgvyU5ifn0xZdhJRETov4CbPXDlLRIJTTKSfJUWpLClKBQI/FNvf0Mnm6uNsOxLYCawtPwZAVISP2TlJzM9LZsHUwI5A3wa8RUf6IjJqDe097Dhygu1HTrD9SBu76tvp7Q90CWUlxZz5JjA/P4Wy7ERNIz2O1L0jIhOut3+Qvcc62F5zgh21bWyvOXFmBtEov4+ZWQnMyU1mdm4Sc3OTKcmIx6+ZRMdEUIa+Ru+IhJ6mjh62H2ljx5ETfFTXxu76jjPnBiZF+rkkJ5E5ucnMyU1iTm4yBamxurTkRQjK0B+iI32R0DU4aDnU0k15XRvlde2U17Wx52gHp51uocSYCGY7O4C5uUnMzk0mOylGO4Lz0IlcEfEkn89QkhFPSUY8X1iQC0DfwCCVjV2BHUF9YEfwi3WH6B8MHJymxUcxKzuJsuxE55bE1MmxusjMp6DQFxHPiPT7mJWdyKzsRO5w2nr6Btjf0MmuujY+qmtnz9GOj+0I4qMjmJmVwKyswE5gVnYi0zITNHT0LNS9IyJB53T/AJWNXew92sGeo4Edwb5jHXT3DgCBXxOXZiRQ5uxAyrKTmJmVQEJM6F91TN07IhJyoiP8XJKTxCU5SUAeEDhHcLi1mz1HO5xbO+/sb+J32+rOvG9qaizTMxOYkZXIjCkJTJ+SQEFqXFiNHFLoi0hI8PkMRenxFKXHc8vcbCBwrYGmztOBbwP1Hexr6GB/Qydv7WvE6R0iJtJHaUZgBzBjSgIzpiQyfUoC6QnRLq7N+FHoi0jIMsaQmRhDZmIMV8/IPNPe0xfoHtrX0EFFQycVDZ28V9HMC8O+FaTFRzF9SgLTMxOZkRXYIZRmJAT9zKOeDP1h4/TdLkVEQlBMpJ/ZuUnMzk36WHtL12kqGjrZ39DJ/mMdVDR28symmjPDSH0G8ifHUpKRQGlmPKUZ8UzLTKA4PT5odgY6kSsicg4Dg5aa1m4qGjrZ19DJwaZOKhu7qG7pPjOCyBjITZlEaUYCpc4w1NLMBEoy4if8OsU6kSsiMgr+YecKbpiddaa9b2CQmtZuKhu7qGxybo2d/Lmy5cxU1BCYibQkI/CtoDQznpKMwM4gaZI7I4kU+iIiFyHS73MCPIEbhrX3Dwxy5PhJKpu6OOjsCCqbuth4qPVMNxFARkI0RelxFKfHB24Z8RSnx5GdNGlcf2ym0BcRGUMRft+ZbwbXlf2lfWDQUn/iFJVNnRxo7OJQcxdVzV288tFROnr6z7wuJtJHYVo8a76xdFx+V6DQFxGZAH6fIT81lvzUWK6Z+ZeRRNZaWrt7qWrqoqq5m6rmLupOnBy3cwEKfRERFxljSIuPJi0++syFasaTJqcQEQkjCn0RkTDiydA3xtxijHmivb3d7VJEREKKJ0PfWvuKtfbBpKSk879YREQumCdDX0RExodCX0QkjCj0RUTCiEJfRCSMeHqWTWNMM1BzkW9PA1rGsJxgoHUOD+G2zuG2vjC6dZ5qrU0/25OeDv3RMMZsPdf0oqFI6xwewm2dw219YXzXWd07IiJhRKEvIhJGQjn0n3C7ABdoncNDuK1zuK0vjOM6h2yfvoiI/LVQPtIXEZERFPoiImEk5ELfGHO9MabCGHPQGPOI2/WMhjEmzxjzrjFmrzFmjzHmPzrtk40xbxpjKp3/pjjtxhjzU2fdy40xC4Z91r3O6yuNMfe6tU4XyhjjN8bsMMasdR4XGmM2Oev2W2NMlNMe7Tw+6DxfMOwzvue0VxhjrnNnTS6MMSbZGPOCMWa/MWafMWZZqG9nY8x3nX/Xu40xzxljYkJtOxtjVhtjmowxu4e1jdl2NcYsNMbsct7zU2PM+S+ua60NmRvgB6qAIiAK+AiY5XZdo1ifLGCBcz8BOADMAv438IjT/gjwv5z7NwJ/AgywFNjktE8GDjn/TXHup7i9fudZ978DngXWOo/XAHc49x8DHnLu/wfgMef+HcBvnfuznO0fDRQ6/y78bq/XOdb334CvO/ejgORQ3s5ADlANTBq2fe8Lte0MrAAWALuHtY3ZdgU2O681zntvOG9Nbv9RxvgPvAx4fdjj7wHfc7uuMVy/PwKfBSqALKctC6hw7j8OrBr2+grn+VXA48PaP/Y6r92AXOBt4GpgrfMPugWIGLmdgdeBZc79COd1ZuS2H/46r92AJCcAzYj2kN3OTujXOkEW4Wzn60JxOwMFI0J/TLar89z+Ye0fe93ZbqHWvTP0D2lIndMW9Jyvs/OBTUCmtfaY81QDMHSV5bOtf7D9XR4F/gsw6DxOBdqstf3O4+H1n1k35/l25/XBtM6FQDPwK6dL65fGmDhCeDtba+uBHwJHgGMEtts2Qns7Dxmr7Zrj3B/Zfk6hFvohyRgTD7wIfMda2zH8ORvYxYfMuFtjzM1Ak7V2m9u1TKAIAl0AP7fWzge6CXztPyMEt3MKcBuBHV42EAdc72pRLnBju4Za6NcDecMe5zptQcsYE0kg8J+x1r7kNDcaY7Kc57OAJqf9bOsfTH+Xy4BbjTGHgecJdPH8BEg2xkQ4rxle/5l1c55PAloJrnWuA+qstZucxy8Q2AmE8na+Fqi21jZba/uAlwhs+1DezkPGarvWO/dHtp9TqIX+FqDUGQEQReCEz8su13TRnDPxTwL7rLU/HvbUy8DQGfx7CfT1D7Xf44wCWAq0O18jXwdWGmNSnCOslU6b51hrv2etzbXWFhDYfu9Ya+8E3gW+6Lxs5DoP/S2+6LzeOu13OKM+CoFSAie9PMda2wDUGmOmO03XAHsJ4e1MoFtnqTEm1vl3PrTOIbudhxmT7eo812GMWer8De8Z9lln5/ZJjnE4aXIjgVEuVcDfu13PKNflcgJf/cqBnc7tRgJ9mW8DlcBbwGTn9Qb4mbPuu4BFwz7ra8BB5/ZVt9ftAtf/M/xl9E4Rgf+ZDwK/A6Kd9hjn8UHn+aJh7/97529RwQWManB5XecBW51t/QcCozRCejsD/wTsB3YDTxEYgRNS2xl4jsA5iz4C3+juH8vtCixy/n5VwP9lxGCAT7ppGgYRkTASat07IiJyDgp9Ea4SE0YAAAAkSURBVJEwotAXEQkjCn0RkTCi0BcRCSMKfRGRMKLQFxEJI/8fmexqI0qyOuoAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "losses = np.zeros(epochs)\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    hidden_activations, output_activations, loss = forward(X_train)\n",
        "    backward(X_train, y_train, hidden_activations, output_activations)\n",
        "    # save & print the loss\n",
        "    losses[epoch] = loss\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}: loss = {loss}\")\n",
        "# semilogy of the loss\n",
        "plt.semilogy(losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heLtyuTIEAqj"
      },
      "source": [
        "Test your model & plot its outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "I_DP1fRd79-q",
        "outputId": "fbaef4bc-7385-44dd-b329-b53fccb5a772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.03540787]\n",
            " [0.96868017]\n",
            " [0.97319289]\n",
            " [0.0212762 ]]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAD8CAYAAAABraMFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXbUlEQVR4nO3df5BmVX3n8feHHoGKSpxhCMwOP4TKmAWDAplCLFKKEWG0tsAqXTNksw4p2FmzIdnSNbVQbIGFZdVoatetVHClV2dBk/AjZNXOihLkR7G1CE67Isi4wDBGmREdmUGyLgbo7s/+cW+Tx6af7tvz3NvPffp+XtSpvj/Ovc+5UHzr3HPuOUe2iYjomkOGXYCIiGFI8IuITkrwi4hOSvCLiE5K8IuITkrwi4hOGij4SVoj6Q5Jj5d/V/fJNy3pwTJN9Bw/UdIDknZJulnSoYOUJyKiqkFrfpcDd9reANxZ7s/n57ZPK9MFPcc/DnzS9q8CzwCXDFieiIhKNMhHzpIeBc6x/ZSkdcA9tn9tnnw/s/2qOccE/AQ4xvaUpDcDH7F9/kEXKCKiolUDXn+07afK7R8BR/fJd7ikSWAK2Gb7i8CRwE9tT5V59gDr+/2QpK3AVoAxxn7jl3TEgEWP5bTh1P837CLEEnz/ySmePjCtQe5x/tte6f0Hpivl/eZDz99ue9Mgv7dUiwY/SV8Djpnn1JW9O7YtqV818gTbeyWdBNwl6WHg2aUU1PY4MA5wxCFrfNaqVBBHyZe/+o1hFyGW4KxNewa+x/4D03zj9uMr5R1b9/jagX9wiRYNfrbP7XdO0o8lret57d3X5x57y7+7Jd0DnA78NfAaSavK2t+xwN6DeIaIaCEDM8wMuxh9DdrhMQFsKbe3AF+am0HSakmHldtrgbOBnS4aG+8G3rvQ9RExmox50dOV0jAMGvy2Ae+Q9DhwbrmPpI2SPlPmORmYlPRtimC3zfbO8ty/Bz4kaRdFG+BnByxPRLTITMV/hmGgDg/b+4G3z3N8Eri03L4POLXP9buBMwcpQ0S0kzHTLZ4yb9De3oiIvmZI8IuIjjEwneAXEV2Uml9EdI6BF9PmFxFdY5zX3ojoIMN0e2Nfgl9ENKMY4dFeCX4R0RAxzUBzIzQqwS8iGlF0eCT4RUTHFN/5JfhFRAfNpOYXEV2Tml9EdJIR0y1eIDLBLyIak9feiOgcI17w2LCL0VeCX0Q0ovjIOa+9EdFB6fCIiM6xxbTbW/MbqGSS1ki6Q9Lj5d/V8+Q5TdLXJT0i6SFJv91z7npJ35P0YJlOG6Q8EdEuM6hSGoZBw/LlwJ22NwB3lvtzPQe83/brgU3Af5b0mp7zf2z7tDI9OGB5IqIlig6PVZXSMAwa/C4Ebii3bwDePTeD7cdsP15u/5Bibd+jBvzdiGi52Q6PKmkYBv3Vo20/VW7/CDh6ocySzgQOBZ7oOfyx8nX4k7Pr+0bEyjBtVUrDsGh9U9LXgGPmOXVl745tS+o7daGkdcDngS22Z6f5uoIiaB4KjFOs43tNn+u3AlsBDueXFit2RAzZyI/wsH1uv3OSfixpne2nyuC2r0++I4AvA1favr/n3rO1xucl/TfgwwuUY5wiQHLEIWtaPD9sRMyaWam9vcAEsKXc3gJ8aW4GSYcCXwA+Z/vWOefWlX9F0V74nQHLExEtUUxscEilNAyDdrNsA26RdAnwfeB9AJI2Ah+wfWl57C3AkZIuLq+7uOzZ/QtJRwECHgQ+MGB5IqIljHhxpQ5vs70fePs8xyeBS8vtPwf+vM/1vzXI70dEe9m0+iPnjPCIiIYM7wPmKhL8IqIRJjW/iOiokf7UJSLiYBi1ejLT9obliBhpxdKVqyqlKiRtkvSopF2SXjaPgKTjJd0t6VvlqLF3LXS/BL+IaEixaHmVtOidpDHgWuCdwCnARZJOmZPtPwC32D4d2Ax8aqF75rU3Ihphah3hcSawy/ZuAEk3UUyssnPOTx5Rbv8y8MOFbpjgFxGNWcJMzmslTfbsj5dDWmetB57s2d8DvGnOPT4C/K2kPwReCfQdmgsJfhHREFtLqfk9bXvjgD95EXC97f8o6c3A5yX9es9EKr8gwS8iGlF0eNQ2vG0vcFzP/rHlsV6XUEyYjO2vSzocWEufCVfS4RERDSnW8KiSKtgBbJB0YjlZymaKiVV6/YByuK2kk4HDgZ/0u2FqfhHRiKLDo57v/GxPSboMuB0YA7bbfkTSNcCk7Qng3wH/VdIHy5+/2Hbf6e8S/CKiMXWO8LB9G3DbnGNX9WzvBM6uer8Ev4hoRNtHeCT4RURjhrU4URUJfhHRCBtenEnwi4iOKV57E/wiooOWMMJj2SX4RUQj6vzUpQm11EkrTDVzmKSby/MPSHptz7kryuOPSjq/jvJERBsUr71V0jAM/KsVp5q5BHjG9q8CnwQ+Xl57CsWX2q+nGJbyqfJ+EbECzJTreCyWhqGOkPvSVDO2XwBmp5rpdSFwQ7l9K/D2cq3eC4GbbD9v+3vArvJ+ETHiit7esUppGOoIfvNNNbO+Xx7bU8CzwJEVrwVA0lZJk5ImX/TzNRQ7Ipo0+5FzlTQMI9PhUc7tNQ5wxCFr+o7Xi4j2WOlLV1aZamY2zx5JqyhmWd1f8dqIGEFd6O2tMtXMBLCl3H4vcFc528IEsLnsDT4R2AB8o4YyRUQLtLm3d+CaX8WpZj5LMavqLuAARYCkzHcLxTz8U8Af2J4etEwRMXy2mFrpIzwqTDXzD8A/73Ptx4CP1VGOiGiXNr/2jkyHR0SMlra3+SX4RURjEvwionMymWlEdNZK/84vIuJlbJjKZKYR0UV57Y2IzkmbX0R0lhP8IqKL0uEREZ1jp80vIjpJTKe3NyK6KG1+EdE5GdsbEd3kot2vrRL8IqIx6e2NiM5xOjwioqvy2hsRndTm3t5a6qSSNkl6VNIuSZfPc/5DknZKekjSnZJO6Dk3LenBMs1d+CgiRpRdBL8qaRgGrvlJGgOuBd5Bsej4DkkTtnf2ZPsWsNH2c5J+H/gE8NvluZ/bPm3QckRE+7T5U5c6an5nArts77b9AnATcGFvBtt3236u3L2fYn3eiFjh7GppGOoIfuuBJ3v295TH+rkE+ErP/uGSJiXdL+nd/S6StLXMN/minx+sxBHROCNmZg6plKpYrHmtzPO+sontEUl/udD9lrXDQ9LvAhuBt/YcPsH2XkknAXdJetj2E3OvtT0OjAMccciaFvchRcSsuv5HrdK8JmkDcAVwtu1nJP3KQveso+a3FziuZ//Y8tjcwp8LXAlcYP9j1c323vLvbuAe4PQayhQRw1Zvh8eizWvAvwKutf0MgO19C92wjuC3A9gg6URJhwKbgV/otZV0OnAdReDb13N8taTDyu21wNlAb0dJRIwyV0yLq9K89jrgdZL+V9mMtmmhGw782mt7StJlwO3AGLDd9iOSrgEmbU8AfwK8CvgrSQA/sH0BcDJwnaQZikC8bU4vcUSMsCV8xrJW0mTP/njZ1LUUq4ANwDkUb6D3SjrV9k/7ZR6Y7duA2+Ycu6pn+9w+190HnFpHGSKiXQzMzFQOfk/b3rjA+SrNa3uAB2y/CHxP0mMUwXDHfDds78C7iBhtBqxqaXGLNq8BX6So9c02o70O2N3vhgl+EdGYur7zsz0FzDavfRe4ZbZ5TdIFZbbbgf2SdgJ3A39se3+/e2Zsb0Q0p8aP0io0rxn4UJkWleAXEQ0Z3rjdKhL8IqI5LR6OkOAXEc0wuHpv77JL8IuIBiX4RUQX5bU3IjopwS8iOmf2I+eWSvCLiMZkAaOI6Kb09kZEFyk1v4jonOpz9Q1Fgl9ENKTyjC1DkeAXEc1JzS8iOmlm2AXoL8EvIprR8u/8apnMdLH1NCVdLOknkh4s06U957ZIerxMW+ooT0S0g1wtDcPANb8q62mWbrZ92Zxr1wBXU6zla+Cb5bXPDFquiGiBFrf51VHzq7KeZj/nA3fYPlAGvDuABZebi4ioQx1tfvOtp/mmefK9R9JbgMeAD9p+ss+1c9fiBEDSVmArwPHrV/HVycn5skVLnf9Pzhh2EWIJHq/p5avNHzkv1wJGfwO81vYbKGp3Nyz1BrbHbW+0vfGoI8dqL2BE1MwUw9uqpCGoI/gtup6m7f22ny93PwP8RtVrI2KEuWIagjqC36LraUpa17N7AcXSc1AsNXeepNWSVgPnlcciYgVY0b29tqckza6nOQZsn11PE5i0PQH8Ubm25hRwALi4vPaApI/yjyuqX2P7wKBlioiWaHGbXy0fOVdYT/MK4Io+124HttdRjohomZUe/CIi5hrmK20VCX4R0ZxMZhoRXZSaX0R0U4JfRHRO2vwiorMS/CKii9TiyUyXa2xvRESrpOYXEc3Ja29EdE46PCKisxL8IqKTEvwiomtEensjoosqzuVXtV1wsVUie/K9R5IlbVzofgl+EdGcmmZy7lkl8p3AKcBFkk6ZJ9+rgX8LPLDYPRP8IqI59U1jX3WVyI8CHwf+YbEbJvhFRGOW8Nq7VtJkT9o651aLrvQo6QzgONtfrlK2dHhERHOq9/Y+bXvBNrqFSDoE+E+US2RUUUvNb7GGSEmflPRgmR6T9NOec9M95ybmXhsRI8pFb2+VVMFiKz2+Gvh14B5JfwecBUws1OkxcM2vpyHyHRRV0R2SJmzvnM1j+4M9+f8QOL3nFj+3fdqg5YiIFqrvO7+XVomkCHqbgd956WfsZ4G1s/uS7gE+bHuy3w3rqPlVbYicdRFwYw2/GxEtV9enLrangNlVIr8L3DK7SmS5MuSS1dHmN19D5JvmyyjpBOBE4K6ew4dLmqRY1nKb7S/2uXYrsBXg+PVpqowYCTWO8Fhslcg5x89Z7H7LHUU2A7fanu45doLtvZJOAu6S9LDtJ+ZeaHscGAfY+MbDWzxoJiKApXzGMhR1vPYu1hDZazNzXnlt7y3/7gbu4RfbAyNiRIl6R3jUrY7g91JDpKRDKQLcy3ptJf1TYDXw9Z5jqyUdVm6vBc4Gds69NiJGU5uD38CvvbanJM02RI4B22cbIoFJ27OBcDNwk+3eRz0ZuE7SDEUg3tbbSxwRI67Fr721tPlVaYi0/ZF5rrsPOLWOMkREC6304BcR8TKZyTkiOivBLyK6qM2TmSb4RURj8tobEd3T8o+cE/wiojkJfhHRNbMjPNoqwS8iGqOZ9ka/BL+IaEba/CKiq/LaGxHdlOAXEV2Uml9EdFOCX0R0jjO8LSI6KN/5RUR3ub3RL8EvIhqTml9EdE/LP3KuYwEjJG2XtE/Sd/qcl6Q/lbRL0kOSzug5t0XS42XaUkd5IqIdNFMtDUMtwQ+4Hti0wPl3AhvKtBX4LwCS1gBXUyxyfiZwtaTVNZUpIoZsxQc/2/cCBxbIciHwORfuB14jaR1wPnCH7QO2nwHuYOEgGhGjwhQdHlXSECxXm9964Mme/T3lsX7HX0bSVopaI8evT1NlxChoc4dHXa+9jbM9bnuj7Y1HHTk27OJERBWumIZguYLfXuC4nv1jy2P9jkfEiJv9yLlKGoblCn4TwPvLXt+zgGdtPwXcDpwnaXXZ0XFeeSwiRp2NZqqlYail8UzSjcA5wFpJeyh6cF8BYPvTwG3Au4BdwHPA75XnDkj6KLCjvNU1thfqOImIUdLiNr9agp/tixY5b+AP+pzbDmyvoxwR0S5t7vBIt2lENMNA1vCIiE5qb+xL8IuI5uS1NyI6KUtXRkT3dGFWl4iIuYqPnF0pVbqftEnSo+XsUJfPc/5DknaWM0fdKemEhe6X4BcRzZmpmBYhaQy4lmKGqFOAiySdMifbt4CNtt8A3Ap8YqF7JvhFRGNqrPmdCeyyvdv2C8BNFLNFvcT23bafK3fvpxgu21eCX0Q0o+qkBkXsWytpsidtnXO3yjNAlS4BvrJQ8dLhERENWdK43adtb6zjVyX9LrAReOtC+RL8IqI59U1UWmkGKEnnAlcCb7X9/EI3TPCLiGbUu2j5DmCDpBMpgt5m4Hd6M0g6HbgO2GR732I3TPCLiObUVPOzPSXpMoop78aA7bYfkXQNMGl7AvgT4FXAX0kC+IHtC/rdM8EvIppT40fOtm+jmB6v99hVPdvnLuV+CX4R0RjNDGlptgoS/CKiGabSB8zDkuAXEY0Q1YeuDUOCX0Q0p8XBr5YRHpK2S9on6Tt9zv+LcrDxw5Luk/TGnnN/Vx5/UNJkHeWJiJZo8aLldQ1vux7YtMD571F8dHgq8FFgfM75t9k+ra4vvCOiBWbb/GqY2KAJdS1gdK+k1y5w/r6e3UUHHEfEytDm3t5hTGwwd8Cxgb+V9M15BjNHxMiq+Mo7pNfeZe3wkPQ2iuD3mz2Hf9P2Xkm/Atwh6f/Yvneea7cCWwGOX59+mojWMyu/w6MKSW8APgNcaHv/7HHbe8u/+4AvUMzb9TK2x21vtL3xqCPHlqPIETGoFrf5LUvwk3Q88N+Bf2n7sZ7jr5T06tlt4Dxg3h7jiBg9dU5jX7da3h8l3QicQzEh4R7gauAVALY/DVwFHAl8qhxwPFX27B4NfKE8tgr4S9tfraNMEdECLX7trau396JFzl8KXDrP8d3AG19+RUSMPBum29vbm56DiGjOSq/5RUTMK8EvIjrHQPU1PJZdgl9ENMTgtPlFRNeYdHhEREelzS8iOinBLyK6Z3iTFlSR4BcRzTDQ4imtEvwiojmp+UVE92R4W0R0kcH5zi8iOikjPCKik9LmFxGdY6e3NyI6KjW/iOge4+npYReirwS/iGhGprSKiM5q8acutazeJmm7pH2S5l15TdI5kp6V9GCZruo5t0nSo5J2Sbq8jvJExPAZ8IwrpWGoq+Z3PfBnwOcWyPM/bf+z3gOSxoBrgXcAe4AdkiZs76ypXBExLO7AZKa275X02oO49ExgV7mKG5JuAi4EEvwiVoB0eBTeLOnbwA+BD9t+BFgPPNmTZw/wpvkulrQV2FruPj+2btdKXNx8LfD0sAvRjF0r9dlW6nP92qA3+L88c/vXfOvaitmX/d/hcgW//w2cYPtnkt4FfBHYsJQb2B4HxgEkTZaLnq8oK/W5YOU+20p+rkHvYXtTHWVpSi0dHoux/fe2f1Zu3wa8QtJaYC9wXE/WY8tjERGNWpbgJ+kYSSq3zyx/dz+wA9gg6URJhwKbgYnlKFNEdFstr72SbgTOAdZK2gNcDbwCwPangfcCvy9pCvg5sNm2gSlJlwG3A2PA9rItcDHjdZS7hVbqc8HKfbY814iSWzz2LiKiKcvy2hsR0TYJfhHRSSMR/CStkXSHpMfLv6v75JvuGULX2o6TxYb0STpM0s3l+QcO8gPyZVfhuS6W9JOe/0aXDqOcS1Vh+KYk/Wn53A9JOmO5y3gwBhmWuiLYbn0CPgFcXm5fDny8T76fDbusFZ5lDHgCOAk4FPg2cMqcPP8G+HS5vRm4edjlrum5Lgb+bNhlPYhnewtwBvCdPuffBXwFEHAW8MCwy1zTc50D/I9hl7OpNBI1P4ohbzeU2zcA7x5iWQb10pA+2y8As0P6evU+763A22c/FWqxKs81kmzfCxxYIMuFwOdcuB94jaR1y1O6g1fhuVa0UQl+R9t+qtz+EXB0n3yHS5qUdL+ktgbI+Yb0re+Xx/YU8Cxw5LKU7uBVeS6A95SvhrdKOm6e86Oo6rOPojdL+rakr0h6/bALU6fWzOcn6WvAMfOcurJ3x7Yl9fs+5wTbeyWdBNwl6WHbT9Rd1jhofwPcaPt5Sf+aonb7W0MuU/Q38LDUNmtN8LN9br9zkn4saZ3tp8rXiX197rG3/Ltb0j3A6RTtUG1SZUjfbJ49klYBv0wxIqbNFn0u273P8BmKttyVYEUO07T99z3bt0n6lKS1tlfERA6j8to7AWwpt7cAX5qbQdJqSYeV22uBs2nn1FhVhvT1Pu97gbtctkC32KLPNacd7ALgu8tYviZNAO8ve33PAp7taaYZWQsMS10RWlPzW8Q24BZJlwDfB94HIGkj8AHblwInA9dJmqH4j7TNLZwU1fa8Q/okXQNM2p4APgt8XtIuigbpzcMrcTUVn+uPJF0ATFE818VDK/ASVBi+eRtFj+8u4Dng94ZT0qUZYFjqipDhbRHRSaPy2hsRUasEv4jopAS/iOikBL+I6KQEv4jopAS/iOikBL+I6KT/D2LynCy7cSkhAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "_, y_pred, _ = forward(X_train)\n",
        "print(y_pred)\n",
        "plt.imshow(y_pred.reshape(2,2))\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OHJtSXSF97G"
      },
      "source": [
        "Re-build your model using $ReLU$ activation instead of sigmoid (same number of hidden units, you may need to adjust the learning rate to $0.01$); which activation function converges the fastest?\n",
        "\n",
        "If one activation function converges faster, does that mean it is better? No, to determine which activation function is better you would need to compare the loss on the validation set (our testing set). And then use the test set to announce your unbiased loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "kWjJJwplJxAI",
        "outputId": "04080190-eaa2-46c6-9958-90637c3aa23e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: loss = [2.36597838]\n",
            "Epoch 1000: loss = [0.01512198]\n",
            "Epoch 2000: loss = [0.00182166]\n",
            "Epoch 3000: loss = [0.00018429]\n",
            "Epoch 4000: loss = [1.74385852e-05]\n",
            "Epoch 5000: loss = [1.61554701e-06]\n",
            "Epoch 6000: loss = [1.48663182e-07]\n",
            "Epoch 7000: loss = [1.36519488e-08]\n",
            "Epoch 8000: loss = [1.25289573e-09]\n",
            "Epoch 9000: loss = [1.14960983e-10]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# weight and bias matrices for the hidden layer\n",
        "W_hidden = np.random.randn(n_hidden, 2)  # n_hidden rows and two columns, we \n",
        "# are therefore doing W*x with x a column vector (to match the slides).\n",
        "# The same would obviously work with x*W with x a line vector.\n",
        "b_hidden = np.zeros((n_hidden))  # b_hidden can be considered as a vector, \n",
        "# a matrix would work but isn't necessary. It will have to be transposed\n",
        "# as vectors are line vectors in Python.\n",
        "\n",
        "# weight and bias matrices for the output layer\n",
        "W_out = np.random.randn(1, n_hidden)\n",
        "b_out = np.zeros((1))\n",
        "\n",
        "\n",
        "# number of hidden units\n",
        "n_hidden = 10\n",
        "# learning rate\n",
        "lr = 0.001\n",
        "# number of epochs\n",
        "epochs = 10_000\n",
        "\n",
        "\n",
        "# activation function\n",
        "def relu(x):\n",
        "    return x.clip(0,None)\n",
        "# and its derivative\n",
        "def relu_derivative(x):\n",
        "    return 1*(x>=0)\n",
        "\n",
        "\n",
        "def forward(X):\n",
        "    # number of samples\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # hidden layer activations\n",
        "    hidden_activations = np.zeros((n_samples, W_hidden.shape[0]))\n",
        "    # output layer activations\n",
        "    output_activations = np.zeros((n_samples, W_out.shape[0]))\n",
        "    # loss\n",
        "    loss = 0\n",
        "    for i in range(n_samples):\n",
        "        hidden_activations[i] = relu(np.dot(W_hidden, X[i].T) + b_hidden.T)  # layer 1\n",
        "        output_activations[i] = relu(np.dot(W_out, hidden_activations[i]) + b_out)  # y_hat\n",
        "        loss += (y_train[i] - output_activations[i]) ** 2\n",
        "    return hidden_activations, output_activations, loss\n",
        "\n",
        "    # It's also important to note that this solution is not the most efficient, \n",
        "    # especially when you have a large dataset. But for the sake of clarity,\n",
        "    # it is more understandable if we iterate over the samples in X to calculate\n",
        "    # the hidden_activations and output_activations for each sample.\n",
        "\n",
        "def backward(X, y_train, hidden_activations, output_activations):\n",
        "    # global variables because we are going to change them and we need the\n",
        "    # change to only be done locally\n",
        "    global W_hidden, b_hidden, W_out, b_out\n",
        "\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # update weight matrices and biases\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_hidden):\n",
        "            W_out[0][j] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * hidden_activations[i][j]\n",
        "        b_out -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out)\n",
        "\n",
        "        for j in range(n_hidden):\n",
        "            for k in range(2):  # because W_hidden is a matrix n_hidden rows, 2 columns\n",
        "                W_hidden[j][k] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j] * relu_derivative(np.dot(W_hidden, X[i].T) + b_hidden.T)[j] * X[i][k]\n",
        "            b_hidden[j] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j]\n",
        "\n",
        "\n",
        "losses = np.zeros(epochs)\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    hidden_activations, output_activations, loss = forward(X_train)\n",
        "    backward(X_train, y_train, hidden_activations, output_activations)\n",
        "    # save & print the loss\n",
        "    losses[epoch] = loss\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}: loss = {loss}\")\n",
        "# semilogy of the loss\n",
        "plt.semilogy(losses)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRI2jEPoRmiK"
      },
      "source": [
        "Now launch this code. What happens? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "fXJ9UtEpMXsy",
        "outputId": "f329077f-9df7-490d-8dc6-f263d92fcc5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: loss = [2.]\n",
            "Epoch 1000: loss = [2.]\n",
            "Epoch 2000: loss = [2.]\n",
            "Epoch 3000: loss = [2.]\n",
            "Epoch 4000: loss = [2.]\n",
            "Epoch 5000: loss = [2.]\n",
            "Epoch 6000: loss = [2.]\n",
            "Epoch 7000: loss = [2.]\n",
            "Epoch 8000: loss = [2.]\n",
            "Epoch 9000: loss = [2.]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "np.random.seed(44)\n",
        "\n",
        "# weight and bias matrices for the hidden layer\n",
        "W_hidden = np.random.randn(n_hidden, 2)  # n_hidden rows and two columns, we \n",
        "# are therefore doing W*x with x a column vector (to match the slides).\n",
        "# The same would obviously work with x*W with x a line vector.\n",
        "b_hidden = np.zeros((n_hidden))  # b_hidden can be considered as a vector, \n",
        "# a matrix would work but isn't necessary. It will have to be transposed\n",
        "# as vectors are line vectors in Python.\n",
        "\n",
        "# weight and bias matrices for the output layer\n",
        "W_out = np.random.randn(1, n_hidden)\n",
        "b_out = np.zeros((1))\n",
        "\n",
        "\n",
        "# number of hidden units\n",
        "n_hidden = 10\n",
        "# learning rate\n",
        "lr = 0.001\n",
        "# number of epochs\n",
        "epochs = 10_000\n",
        "\n",
        "\n",
        "# activation function\n",
        "def relu(x):\n",
        "    return x.clip(0,None)\n",
        "# and its derivative\n",
        "def relu_derivative(x):\n",
        "    return 1*(x>=0)\n",
        "\n",
        "\n",
        "def forward(X):\n",
        "    # number of samples\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # hidden layer activations\n",
        "    hidden_activations = np.zeros((n_samples, W_hidden.shape[0]))\n",
        "    # output layer activations\n",
        "    output_activations = np.zeros((n_samples, W_out.shape[0]))\n",
        "    # loss\n",
        "    loss = 0\n",
        "    for i in range(n_samples):\n",
        "        hidden_activations[i] = relu(np.dot(W_hidden, X[i].T) + b_hidden.T)  # layer 1\n",
        "        output_activations[i] = relu(np.dot(W_out, hidden_activations[i]) + b_out)  # y_hat\n",
        "        loss += (y_train[i] - output_activations[i]) ** 2\n",
        "    return hidden_activations, output_activations, loss\n",
        "\n",
        "    # It's also important to note that this solution is not the most efficient, \n",
        "    # especially when you have a large dataset. But for the sake of clarity,\n",
        "    # it is more understandable if we iterate over the samples in X to calculate\n",
        "    # the hidden_activations and output_activations for each sample.\n",
        "\n",
        "def backward(X, y_train, hidden_activations, output_activations):\n",
        "    # global variables because we are going to change them and we need the\n",
        "    # change to only be done locally\n",
        "    global W_hidden, b_hidden, W_out, b_out\n",
        "\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # update weight matrices and biases\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_hidden):\n",
        "            W_out[0][j] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * hidden_activations[i][j]\n",
        "        b_out -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out)\n",
        "\n",
        "        for j in range(n_hidden):\n",
        "            for k in range(2):  # because W_hidden is a matrix n_hidden rows, 2 columns\n",
        "                W_hidden[j][k] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j] * relu_derivative(np.dot(W_hidden, X[i].T) + b_hidden.T)[j] * X[i][k]\n",
        "            b_hidden[j] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j]\n",
        "\n",
        "\n",
        "losses = np.zeros(epochs)\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    hidden_activations, output_activations, loss = forward(X_train)\n",
        "    backward(X_train, y_train, hidden_activations, output_activations)\n",
        "    # save & print the loss\n",
        "    losses[epoch] = loss\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}: loss = {loss}\")\n",
        "# semilogy of the loss\n",
        "plt.semilogy(losses)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKTuerETSFfy"
      },
      "source": [
        "It gets stuck at a loss of 2.. \n",
        "\n",
        "It's important to understand that the initialization of the weights and biases can greatly affect the training of a neural network. If the initialization is not done correctly, it can lead to the model getting stuck and not converging. In this case, it appears that the gradients are reaching a value of zero, which means that none of the weights are updating from one iteration to the next. This results in neither the loss nor the parameters changing, and the network not being able to converge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "id": "QaWe5K4GSUy5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "689ffbb94fe8f58a5045b4f3f0726e738a118a8a590ae859861904a2cad8ac3d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
