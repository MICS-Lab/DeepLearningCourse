{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate architectures and advanced PyTorch tools\n",
    "## TD 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are essentially going to use the same `Food101` ([credit where it's due](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)) data, the same object `ImageDataset`, the same `DataLoader`.\n",
    "\n",
    "The code below is mainly a copy of the code from the previous TD, except that global variables are now defined separately and everything is wrapped in different functions. This is to make it easier to train the same model with different hyperparameters and architectures, etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "_ = torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "\n",
    "# Setup device-agnostic code\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size: int = 4) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Load the training and test datasets into data loaders.\n",
    "    \"\"\"\n",
    "    data_dir = pathlib.Path(\"data\")\n",
    "    train_dir = data_dir / \"Food-3\" / \"train\"\n",
    "    test_dir = data_dir / \"Food-3\" / \"test\"\n",
    "\n",
    "    data_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(64, 64)),  # Resize the images to 64x64*\n",
    "            transforms.ToTensor()  # Convert the images to tensors\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_data = datasets.ImageFolder(\n",
    "        root=train_dir,  # target folder of images\n",
    "        transform=data_transform,  # transforms to perform on data (images)\n",
    "        target_transform=None  # transforms to perform on labels (if necessary)\n",
    "    ) \n",
    "\n",
    "    test_data = datasets.ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=data_transform\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,  # how many samples per batch?\n",
    "        shuffle=True  # shuffle the data?\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    ) # don't usually need to shuffle testing data\n",
    "\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataloaders in global variables\n",
    "TRAIN_DATALOADER, TEST_DATALOADER = get_data_loaders(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_units=200):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(64*64*3, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: Net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_our_model() -> float:\n",
    "    # 0. Put model in eval mode\n",
    "    MODEL.eval()  # to remove stuff like dropout that's only going to be in the training part\n",
    "\n",
    "    # 1. Setup test accuracy value\n",
    "    test_acc = 0\n",
    "\n",
    "    # 2. Turn on inference context manager\n",
    "    with torch.no_grad():\n",
    "        # Loop through DataLoader batches\n",
    "        for X_test, y_test in TEST_DATALOADER:  # majuscule Ã  X car c'est une \"matrice\", et y un entier\n",
    "            # a. Move data to device\n",
    "            X_test_flattened = X_test.view(BATCH_SIZE, 64*64*3).to(DEVICE)\n",
    "            y_test = y_test.to(DEVICE)\n",
    "\n",
    "            # b. Forward pass\n",
    "            model_output = MODEL(X_test_flattened)\n",
    "\n",
    "            # c. Calculate and accumulate accuracy\n",
    "            test_pred_label = model_output.argmax(dim=1)\n",
    "            test_acc += (test_pred_label == y_test).sum()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_acc = test_acc / (BATCH_SIZE*len(TEST_DATALOADER))\n",
    "    return test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.00%\n"
     ]
    }
   ],
   "source": [
    "# Test our untrained model\n",
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 36.00% accuracy on the testing set without training and with the default hyperparameters if you used the same seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(loss_fn, optimizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and modified the trained model inplace.\n",
    "    \"\"\"\n",
    "    start_time_global = time.time()\n",
    "\n",
    "    # Put model in train mode\n",
    "    MODEL.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time_epoch = time.time()\n",
    "        for X, y in TRAIN_DATALOADER:\n",
    "            # 0. Move data to device\n",
    "            X = X.view(BATCH_SIZE, 64*64*3).to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_pred = MODEL(X)\n",
    "\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # 3. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 4. Loss backward\n",
    "            loss.backward()\n",
    "\n",
    "            # 5. Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate and accumulate accuracy metric across all batches\n",
    "            y_pred_class = y_pred.argmax(dim=1)\n",
    "            train_acc += (y_pred_class == y).sum()\n",
    "\n",
    "        # Adjust metrics to get average loss and accuracy per batch\n",
    "        train_loss = train_loss / (BATCH_SIZE * len(TRAIN_DATALOADER))\n",
    "        train_acc = train_acc / (BATCH_SIZE * len(TRAIN_DATALOADER))\n",
    "        print(\n",
    "            f\"epoch {epoch+1}/{NUM_EPOCHS},\"\n",
    "            f\" train_loss = {train_loss:.2e},\"\n",
    "            f\" train_acc = {100*train_acc.item():.2f}%,\"\n",
    "            f\" time spent during this epoch = {time.time() - start_time_epoch:.2f}s\"\n",
    "            f\" total time spent = {time.time() - start_time_global:.2f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, train_loss = 2.43e-01, train_acc = 51.00%, time spent during this epoch = 11.76s total time spent = 11.76s\n",
      "epoch 2/10, train_loss = 2.26e-01, train_acc = 56.43%, time spent during this epoch = 11.95s total time spent = 23.71s\n",
      "epoch 3/10, train_loss = 2.19e-01, train_acc = 59.21%, time spent during this epoch = 11.53s total time spent = 35.24s\n",
      "epoch 4/10, train_loss = 2.15e-01, train_acc = 60.02%, time spent during this epoch = 11.60s total time spent = 46.84s\n",
      "epoch 5/10, train_loss = 2.10e-01, train_acc = 61.54%, time spent during this epoch = 11.48s total time spent = 58.32s\n",
      "epoch 6/10, train_loss = 2.07e-01, train_acc = 62.73%, time spent during this epoch = 11.43s total time spent = 69.75s\n",
      "epoch 7/10, train_loss = 2.02e-01, train_acc = 63.76%, time spent during this epoch = 11.49s total time spent = 81.24s\n",
      "epoch 8/10, train_loss = 2.00e-01, train_acc = 64.73%, time spent during this epoch = 12.20s total time spent = 93.44s\n",
      "epoch 9/10, train_loss = 1.96e-01, train_acc = 64.99%, time spent during this epoch = 11.45s total time spent = 104.89s\n",
      "epoch 10/10, train_loss = 1.93e-01, train_acc = 65.51%, time spent during this epoch = 11.54s total time spent = 116.43s\n"
     ]
    }
   ],
   "source": [
    "main_train(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.33%\n"
     ]
    }
   ],
   "source": [
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 58.33% accuracy on the testing set without training and with the default hyperparameters if you used the same seed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to improve this accuracy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install the Optuna package (`pip install optuna`) and import it at the beginning of your script. Then, you will need to define a new function that will be used as the objective function for Optuna's optimization. This function should take in the `trial` object from Optuna as an argument and use the `trial` object to define and sample the hyperparameters that you want to optimize. For example, you can use the `trial` object to sample a choice between a convolutional and dense network, and to sample the number of neurons for the chosen network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this together, and then you'll have to implement optimization of the learning rate and optimizer's choice on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main_train function, we will need to use the sampled hyperparameters to define the architecture of the model, and to pass it to the optimizer. After training the model, we will need to return the final validation loss or accuracy as the objective function value for Optuna.\n",
    "\n",
    "Finally, we will need to call the `optuna.create_study()` function to create a new study, and use the `study.optimize()` function to run the optimization, passing the objective function that we defined earlier.\n",
    "\n",
    "You can find more information about how to use Optuna in the [Optuna documentation](https://optuna.org/docs/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a61f982c8ae83496d3304782998ac96a47f5fcf9bfc174247e19a8162c5490e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
