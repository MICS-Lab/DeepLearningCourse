{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ovDJUER3rFK4"
      },
      "source": [
        "# TD 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmJfTThvBnc"
      },
      "source": [
        "[Use pytorch for all questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R9k-Z90ivZfW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\__APP__\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Width vs Depth"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal here is to compare the performances of basic networks.\n",
        "We will create both very wide and very deep networks, and see which ones are better."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will try to fit a sequence of functions with increasing complexity, both with a wide and with a deep network.\n",
        "The first part concentrates on the minimal amount of neurons needed to fit the function with an optimal network (setting the weights and biases manually).\n",
        "Then, the second part studies the training of the same networks, to fit the same functions. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Theory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking the function of interest: $f: \\mathbb{R} \\to \\mathbb{R}(x)$ to be linear by segment, with 4 segments:\n",
        "- $f(x) = 0$ on $\\left] -\\infty, 0 \\right]$\n",
        "- $f(x) = 2x$ on $\\left] 0, \\frac{1}{2} \\right]$\n",
        "- $f(x) = 2-2x$ on $\\left] \\frac{1}{2}, 1 \\right]$\n",
        "- $f(x) = 0$ on $\\left] 1, \\infty \\right]$\n",
        "\n",
        "Define $f(x)$ as a python function, using numpy.\n",
        "\n",
        "Let also:\n",
        "- $g(x, 2) = f(x) \\circ f(x)$\n",
        "- $g(x, 3) = f(x) \\circ f(x) \\circ f(x)$\n",
        "- $g(x, 4) = f(x) \\circ f(x) \\circ f(x) \\circ f(x)$\n",
        "- etc...\n",
        "\n",
        "Define $g(x, l)$ as a python function.\n",
        "\n",
        "Plot $f$ and $g$ on $\\left] -1.2, 1.2 \\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a basic \"rectange\" network class (width is the same in all hidden layers);\n",
        "leave the number of layers and number of neurons per layer as parameters, and use ReLU activation function.\n",
        "The input and output are 1D, since we fit functions $\\mathbb{R} \\to \\mathbb{r}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With 4 hidden layers and 5 neurons per layer, your network class should create a network as follows:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"../images/rectangle_network.svg\" alt=\"rectangle network diagram\" style=\"width: 35em;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement $f$ with a (basic) rectangle network with 1 hidden layer of 3 neurons.\n",
        "Set the weights youself to fit exactly the function.\n",
        "\n",
        "*Hint:*\n",
        "$f(x) = 2x_+ -4(x-\\frac{1}{2})_+ +2(x-1)_+$\n",
        "$\\qquad \\qquad$ (where $\\alpha_+$ is $ReLU(\\alpha)$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, implement $g$ for `level = 4`, by increasing the width (and keeping a single hidden layer).\n",
        "Use again a rectangle network and set the weights youself.\n",
        "\n",
        "*Hint: try to find the weight for `level = 2`, then `level = 3`, and deduce the pattern.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many neurons did you need in the hidden layer? How will that evolve when the level increase?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need $2^{level}+1$ neurons, this increases exponentially with `level`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, implement $g$ for `level = 4`, by increasing the depth (and keeping 3 neuron per hidden layer).\n",
        "Again use a rectangle network and set the weights youself.\n",
        "\n",
        "*Hint: try to find the weight for `level = 2`, then `level = 3`, and deduce the pattern.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How many neurons did you need in the hidden layer? How will that evolve when the level increase?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need $3*{level}$ neurons, this increases linearly with `level`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a semilogy, plot the number of neurons used to replicate $g$ as a function of `level` (ranging from 1 to 15), by increasing the width and the depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a semilogy, plot the number of parameters used to replicate $g$ as a function of `level` (ranging from 1 to 15), by increasing the width and the depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will try to fit `g` with `level = 4` both with deep and wide networks; This time, by training the network, instead of manually setting the weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, train a wide network, of course, it will need more neurons than the mathematically optimal solutions.\n",
        "Try with 5 times more neurons than in the optimal solution, and about $15000$ epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Convergence is not reached, but given a little more time, it should converge nicely towards the solution.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, train a deep network, again, give a little slack on the number of neurons compared to the optimal solution.\n",
        "Try with 10 times more layers than necessart, and 15 neurons per layer instead of 3, and about $5000$ epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- *See that the training takes much more time than with the wide network, despite the fact that we have less epochs.*\n",
        "- *Observe also that there is no sign of convergence; Try to explain this.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The number of neurons is a good measure of the size (in terms of memory) of your network;\n",
        "For the same amount of neurons, deep networks, can catch more complexity, but are harder to train.\n",
        "On the contrary, wide networks catch less complexity, but are easier to train.\n",
        "\n",
        "*Your goal as an AI engineer is to find the best architectures, so that your networks are both trainable and able to catch the complexity of the observed phenomenon.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN: Determinating Lastnames Origins"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal here is to build our first (basic) RNN network."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a datset composed of 18000 names, from 18 nationalities (1000 names from each country).\n",
        "We try to build a network to classify names to their correct nationality.\n",
        "We do that with a RNN, that \"reads\" each letter one by one."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to the dataset *name_1000*, containing 1000 names from 18 nationalities:\n",
        "https://drive.google.com/drive/folders/1qqyB_ZRMsz_7veqlKYnJH2kmYK6myV4Y?usp=share_link\n",
        "\n",
        "Start by downloading it and store it in your working directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some countries are using non-latin alphabet, we need the ASCII version.\n",
        "You can try the function `unidecode` from the `unidecode` mdule.\n",
        "\n",
        "Create a function that takes a name, and return its version using only letters from \n",
        "`LETTERS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'` (you can add some letters if you want, but the more you add, the more complex your network will become)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your function on `'Ślusàrski'`, `'François'`, `'北亰'`, `'Kožušček'`, and `'+-*/'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feeding letters to a network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A network can not, originally, process characters/letters; networks can only understand numbers, and list of numbers.\n",
        "We need to turn our characters to a vector. We could just take the binary byte representing the character in the ASCII. However, this would be very hard for the network to understand (`x: 1011000`,s `y:1011001` and `z: 1011010` will have very similar activations).\n",
        "Thus, we will use 'one-hot encoding' of our set of letters `LETTERS`. That is, we transform each letter to a tensor of size `<1xN_LETTERS>`, where all entries are zero except the one corresponding to the position of the letter, that we set to one.\n",
        "e.g.:\n",
        "- `a => [1, 0, 0, 0, ..., 0]`\n",
        "- `b => [0, 1, 0, 0, ..., 0]`\n",
        "- `c => [0, 0, 1, 0, ..., 0]`\n",
        "\n",
        "Define a `letterToTensor` function that perfoms this operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define a `nameToTensor` function that perfoms this operation for each letter in the name (resulting to a `<name_length x 1 x N_LETTERS>` tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a custom dataset:\n",
        "- in the `__init__`, read all files, and create a list of names and associated country\n",
        "- add a `countryID` method that turns a country its index\n",
        "- add a `countryTensor` method that turns a country to a one-hot encoded tensor\n",
        "- the `__getitem__` should return one piece of data in the form `(name, country, nameTensor, countryID)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split data into train and test with `torch.utils.data.dataset.random_split` (80% for training; 20% for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dataloader for the training and testing datasets; the `batch_size` must be 1, since different names can have different lengths (and therefore, different tensor size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the RNN with the following parameters:\n",
        "- input_size: number of input features\n",
        "- hidden_size: number of hidden units\n",
        "- output_size: number of output features\n",
        "- idx_to_country: list of countries\n",
        "\n",
        "The input is a one-hot vector of size `N_LETTERS`; the output is a one-hot vector of size `N_COUNTRIES = len(idx_to_country)` + a hidden state of size `hidden_size`.\n",
        "You can build the architecture you like, but one that is known to work is the following:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"../images/rnn.svg\" alt=\"rnn architecture\" style=\"width: 35em;\"/>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is, a simple dense layer that takes as input the concatenation of the hiden state and the current letter, and outputs both the new hidden state and a vector of likelyhood for each country. Adding a softmax layer to the countries likelyhood turns them to actual probabilities."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On top of the `__init__` and `forward` methods, define:\n",
        "- `init_hidden` a method that creates a zero hidden state (that we will use as a hidden state when sending the first letter)\n",
        "- `outputToCountry` to convert output probabilities to the corresponding country\n",
        "- `outputToID` to convert output probabilities to the corresponding country ID"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a network with 128 hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feeding the RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feed a single letter to the network (i.e. 1 step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feed a full word to the network (i.e. multiple steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the network; for each iteration of the training:\n",
        "- Create a zero initial hidden state\n",
        "- Feed each letter in and keep hidden state for next letter\n",
        "- Compute the loss\n",
        "- Back-propagate\n",
        "- Zero-out the gradients\n",
        "\n",
        "One configuration that is known to work (for the architecture described above):\n",
        "- Optimizer: Adam\n",
        "- Learning rate: `lr = 0.001`\n",
        "- Loss: Negative log likelihood (`NLLLoss`)\n",
        "- Epoch: No need for too many epoch (~5-10 is enough)\n",
        "\n",
        "NB: if you did not put a softmax, use `CrossEntropyLoss` instead of `NLLLoss`.\n",
        "\n",
        "*(Training takes ~3min on a modern laptop.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on a couple of names from the testing set, display the name, the prediction, and the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on the full test set:\n",
        "- Plot the confusion matrix\n",
        "- Compute the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy on the testing set is ~60%; this is much better than taking a random guess (which will have an accuracy of ~5.5%).\n",
        "\n",
        "You can do you own experiences to improve this result (add layers, test other layers such as LSTM or GRU, try some combinations).\n",
        "\n",
        "You can also change dataset (eg: word -> language; name -> gender; title -> newspaper; etc...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This RNN exercise was inspired from the following notebook of the official PyTorch documentation:\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "689ffbb94fe8f58a5045b4f3f0726e738a118a8a590ae859861904a2cad8ac3d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
